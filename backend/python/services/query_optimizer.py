"""
Ù…Ø­Ø³Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù… - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 4.0
Ù†Ø¸Ø§Ù… ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø´Ø§Ù…Ù„ Ù…Ø¹ ØªØ­Ù„ÙŠÙ„Ø§Øª Ø£Ø¯Ø§Ø¡ Ù…ØªÙ‚Ø¯Ù…Ø© ÙˆØªÙˆØµÙŠØ§Øª Ø°ÙƒÙŠØ©
"""

import os
import logging
import time
import sqlparse
import re
import statistics
import hashlib
from typing import Dict, List, Optional, Any, Tuple, Set
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
from functools import wraps
import threading
from concurrent.futures import ThreadPoolExecutor
import json
import psutil

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø¬Ù„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
logger = logging.getLogger(__name__)

class QueryType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…ÙˆØ³Ø¹Ø©"""
    SELECT = "SELECT"
    INSERT = "INSERT" 
    UPDATE = "UPDATE"
    DELETE = "DELETE"
    JOIN = "JOIN"
    AGGREGATE = "AGGREGATE"
    SUBQUERY = "SUBQUERY"
    COMPLEX_JOIN = "COMPLEX_JOIN"
    WINDOW_FUNCTION = "WINDOW_FUNCTION"
    CTE = "CTE"
    UNION = "UNION"

class OptimizationLevel(Enum):
    """Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ÙˆØ³Ø¹Ø©"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium" 
    LOW = "low"
    OPTIMAL = "optimal"
    EMERGENCY = "emergency"

class IndexType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ÙÙ‡Ø§Ø±Ø³"""
    BTREE = "btree"
    HASH = "hash"
    GIN = "gin"
    GIST = "gist"
    FULLTEXT = "fulltext"

@dataclass
class QueryAnalysis:
    """ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…ØªÙ‚Ø¯Ù…"""
    query_id: str
    query_text: str
    query_type: QueryType
    execution_time: float
    rows_affected: int
    optimization_suggestions: List[str]
    optimization_level: OptimizationLevel
    performance_metrics: Dict[str, Any]
    query_complexity: int
    index_analysis: Dict[str, Any]
    resource_usage: Dict[str, float]
    timestamp: datetime = field(default_factory=datetime.now)

@dataclass
class QueryPattern:
    """Ù†Ù…Ø· Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…ØªÙ‚Ø¯Ù…"""
    pattern_hash: str
    normalized_query: str
    frequency: int
    avg_execution_time: float
    max_execution_time: float
    min_execution_time: float
    optimization_opportunities: List[str]
    last_executed: datetime
    execution_history: List[float] = field(default_factory=list)

@dataclass
class IndexRecommendation:
    """ØªÙˆØµÙŠØ© ÙÙ‡Ø±Ø³ Ù…ØªÙ‚Ø¯Ù…Ø©"""
    table_name: str
    column_names: List[str]
    index_type: IndexType
    expected_benefit: float
    creation_cost: float
    priority: int
    reason: str
    sql_statement: str

@dataclass
class PerformanceReport:
    """ØªÙ‚Ø±ÙŠØ± Ø£Ø¯Ø§Ø¡ Ù…ØªÙƒØ§Ù…Ù„"""
    period_start: datetime
    period_end: datetime
    total_queries: int
    slow_queries: int
    avg_execution_time: float
    performance_distribution: Dict[OptimizationLevel, int]
    top_slow_queries: List[QueryAnalysis]
    index_recommendations: List[IndexRecommendation]
    system_recommendations: List[str]
    resource_utilization: Dict[str, float]

class AdvancedQueryOptimizer:
    """
    Ù…Ø­Ø³Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ ØªØ­Ù„ÙŠÙ„Ø§Øª Ø´Ø§Ù…Ù„Ø© ÙˆØªÙˆØµÙŠØ§Øª Ø°ÙƒÙŠØ©
    """
    
    def __init__(self):
        self.query_history: List[QueryAnalysis] = []
        self.query_patterns: Dict[str, QueryPattern] = {}
        self.performance_thresholds = self._load_performance_thresholds()
        self.optimization_rules = self._load_optimization_rules()
        self.index_recommendations: List[IndexRecommendation] = []
        self.performance_monitor = QueryPerformanceMonitor()
        self.setup_optimizer()
        
        # Ø®Ù„ÙÙŠØ© ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
        self._start_background_cleanup()
    
    def _load_performance_thresholds(self) -> Dict[str, float]:
        """ØªØ­Ù…ÙŠÙ„ Ø¹ØªØ¨Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        return {
            'slow_query_threshold': float(os.getenv('SLOW_QUERY_THRESHOLD', '1.0')),
            'critical_query_threshold': float(os.getenv('CRITICAL_QUERY_THRESHOLD', '5.0')),
            'high_cost_threshold': float(os.getenv('HIGH_COST_THRESHOLD', '1000')),
            'frequent_query_threshold': int(os.getenv('FREQUENT_QUERY_THRESHOLD', '10')),
            'memory_usage_threshold': float(os.getenv('QUERY_MEMORY_THRESHOLD', '100')),  # MB
            'complexity_threshold': int(os.getenv('COMPLEXITY_THRESHOLD', '10'))
        }
    
    def _load_optimization_rules(self) -> Dict[str, Any]:
        """ØªØ­Ù…ÙŠÙ„ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        return {
            'index_optimization': {
                'enabled': True,
                'min_benefit': 0.3,
                'max_indexes_per_table': 5
            },
            'query_restructuring': {
                'enabled': True,
                'complexity_threshold': 5,
                'enable_subquery_to_join': True,
                'enable_predicate_pushdown': True
            },
            'caching_strategy': {
                'enabled': True,
                'frequency_threshold': 5,
                'result_size_threshold': 1000
            },
            'resource_optimization': {
                'enabled': True,
                'memory_usage_threshold': 100,  # MB
                'enable_batch_processing': True
            }
        }
    
    def setup_optimizer(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø­Ø³Ù† Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
        try:
            # ØªØ­Ù…ÙŠÙ„ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ù…Ù† Ù…Ù„Ù Ø¥Ø°Ø§ Ù…ÙˆØ¬ÙˆØ¯
            self._load_optimization_rules_from_file()
            
            # Ø¨Ø¯Ø¡ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡
            self.performance_monitor.start_monitoring()
            
            logger.info("âœ… ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø­Ø³Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø­Ø³Ù†: {e}")
            raise
    
    def _load_optimization_rules_from_file(self):
        """ØªØ­Ù…ÙŠÙ„ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ù…Ù† Ù…Ù„Ù Ø®Ø§Ø±Ø¬ÙŠ"""
        try:
            rules_file = os.getenv('QUERY_OPTIMIZATION_RULES_FILE', 'query_optimization_rules.json')
            if os.path.exists(rules_file):
                with open(rules_file, 'r', encoding='utf-8') as f:
                    external_rules = json.load(f)
                    self.optimization_rules.update(external_rules)
                logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ù…Ù† {rules_file}")
        except Exception as e:
            logger.warning(f"âš ï¸  Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©: {e}")
    
    def _start_background_cleanup(self):
        """Ø¨Ø¯Ø¡ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø®Ù„ÙÙŠØ© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©"""
        def cleanup_old_data():
            while True:
                try:
                    self._cleanup_old_queries()
                    self._cleanup_old_patterns()
                    time.sleep(3600)  # ÙƒÙ„ Ø³Ø§Ø¹Ø©
                except Exception as e:
                    logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
                    time.sleep(300)  # Ø§Ù†ØªØ¸Ø§Ø± 5 Ø¯Ù‚Ø§Ø¦Ù‚ Ø¹Ù†Ø¯ Ø§Ù„Ø®Ø·Ø£
        
        cleanup_thread = threading.Thread(target=cleanup_old_data, daemon=True)
        cleanup_thread.start()
    
    def _cleanup_old_queries(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=24)  # Ø§Ø­ØªÙØ¸ Ø¨ÙŠÙˆÙ… ÙˆØ§Ø­Ø¯
            initial_count = len(self.query_history)
            self.query_history = [
                q for q in self.query_history 
                if q.timestamp > cutoff_time
            ]
            removed_count = initial_count - len(self.query_history)
            if removed_count > 0:
                logger.debug(f"ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ {removed_count} Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù‚Ø¯ÙŠÙ…")
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª: {e}")
    
    def _cleanup_old_patterns(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©"""
        try:
            cutoff_time = datetime.now() - timedelta(days=7)  # Ø§Ø­ØªÙØ¸ Ø¨Ø£Ø³Ø¨ÙˆØ¹
            patterns_to_remove = []
            
            for pattern_hash, pattern in self.query_patterns.items():
                if pattern.last_executed < cutoff_time and pattern.frequency < 5:
                    patterns_to_remove.append(pattern_hash)
            
            for pattern_hash in patterns_to_remove:
                del self.query_patterns[pattern_hash]
            
            if patterns_to_remove:
                logger.debug(f"ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ {len(patterns_to_remove)} Ù†Ù…Ø· Ù‚Ø¯ÙŠÙ…")
                
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ù†Ù…Ø§Ø·: {e}")
    
    def analyze_query(self, query_text: str, execution_time: float, 
                     rows_affected: int = 0, context: Dict = None) -> QueryAnalysis:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ ØªÙ‚ÙŠÙŠÙ… Ø´Ø§Ù…Ù„"""
        try:
            start_analysis_time = time.time()
            
            query_id = self._generate_query_id(query_text)
            query_type = self._classify_query_advanced(query_text)
            
            # ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø£Ø¯Ø§Ø¡
            performance_metrics = self._analyze_query_performance_advanced(
                query_text, execution_time, rows_affected, context
            )
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
            index_analysis = self._analyze_index_usage_advanced(query_text)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
            query_complexity = self._calculate_query_complexity_advanced(query_text)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
            resource_usage = self._analyze_resource_usage(query_text, execution_time, rows_affected)
            
            # Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
            optimization_suggestions = self._generate_advanced_optimization_suggestions(
                query_text, performance_metrics, index_analysis, query_type, query_complexity
            )
            
            # ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ­Ø³ÙŠÙ†
            optimization_level = self._determine_optimization_level_advanced(
                execution_time, performance_metrics, len(optimization_suggestions), query_complexity
            )
            
            analysis = QueryAnalysis(
                query_id=query_id,
                query_text=query_text,
                query_type=query_type,
                execution_time=execution_time,
                rows_affected=rows_affected,
                optimization_suggestions=optimization_suggestions,
                optimization_level=optimization_level,
                performance_metrics=performance_metrics,
                query_complexity=query_complexity,
                index_analysis=index_analysis,
                resource_usage=resource_usage
            )
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            self._update_query_history(analysis)
            self._update_query_patterns(analysis)
            self._update_index_recommendations(analysis, index_analysis)
            
            analysis_time = time.time() - start_analysis_time
            logger.debug(f"â±ï¸  ÙˆÙ‚Øª ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {analysis_time:.4f} Ø«Ø§Ù†ÙŠØ©")
            
            return analysis
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {e}")
            return self._create_error_analysis(query_text, str(e))
    
    def _generate_query_id(self, query_text: str) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø±Ù ÙØ±ÙŠØ¯ Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…"""
        query_hash = hashlib.md5(query_text.encode('utf-8')).hexdigest()[:8]
        timestamp = int(time.time())
        return f"query_{timestamp}_{query_hash}"
    
    def _classify_query_advanced(self, query_text: str) -> QueryType:
        """ØªØµÙ†ÙŠÙ Ù…ØªÙ‚Ø¯Ù… Ù„Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…"""
        try:
            query_upper = query_text.upper().strip()
            parsed = sqlparse.parse(query_text)
            
            if not parsed:
                return QueryType.SELECT
            
            statement = parsed[0]
            first_token = statement.token_first(skip_cm=True, skip_ws=True)
            
            if not first_token:
                return QueryType.SELECT
            
            # ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            if first_token.value.upper() == 'WITH':
                return QueryType.CTE
            
            # ØªØ­Ù„ÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ JOIN
            if any(token.value.upper() == 'JOIN' for token in statement.flatten()):
                join_count = sum(1 for token in statement.flatten() if token.value.upper() == 'JOIN')
                if join_count >= 3:
                    return QueryType.COMPLEX_JOIN
                else:
                    return QueryType.JOIN
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù†Ø§ÙØ°Ø©
            if any('OVER()' in token.value.upper() for token in statement.flatten()):
                return QueryType.WINDOW_FUNCTION
            
            # ØªØ­Ù„ÙŠÙ„ UNION
            if any(token.value.upper() == 'UNION' for token in statement.flatten()):
                return QueryType.UNION
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ÙØ±Ø¹ÙŠØ©
            subquery_count = query_upper.count('SELECT') - 1
            if subquery_count > 0:
                return QueryType.SUBQUERY
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø¬Ù…Ø¹Ø©
            aggregate_functions = ['COUNT(', 'SUM(', 'AVG(', 'MAX(', 'MIN(', 'GROUP_CONCAT(']
            if any(func in query_upper for func in aggregate_functions):
                return QueryType.AGGREGATE
            
            # Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
            query_type_map = {
                'SELECT': QueryType.SELECT,
                'INSERT': QueryType.INSERT,
                'UPDATE': QueryType.UPDATE,
                'DELETE': QueryType.DELETE
            }
            
            return query_type_map.get(first_token.value.upper(), QueryType.SELECT)
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {e}")
            return QueryType.SELECT
    
    def _analyze_query_performance_advanced(self, query_text: str, execution_time: float,
                                          rows_affected: int, context: Dict) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø£Ø¯Ø§Ø¡ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…ØªÙ‚Ø¯Ù…"""
        try:
            metrics = {
                'execution_time': execution_time,
                'rows_affected': rows_affected,
                'query_complexity': self._calculate_query_complexity_advanced(query_text),
                'index_usage_estimate': self._estimate_index_usage_advanced(query_text),
                'potential_bottlenecks': self._identify_advanced_bottlenecks(query_text),
                'memory_estimate': self._estimate_memory_usage_advanced(query_text, rows_affected),
                'io_estimate': self._estimate_io_operations(query_text, rows_affected),
                'is_slow_query': execution_time > self.performance_thresholds['slow_query_threshold'],
                'is_critical_query': execution_time > self.performance_thresholds['critical_query_threshold'],
                'execution_plan_quality': self._estimate_execution_plan_quality(query_text)
            }
            
            # Ø¥Ø¶Ø§ÙØ© Ù…Ù‚Ø§ÙŠÙŠØ³ Ø¥Ø¶Ø§ÙÙŠØ© Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚
            if context:
                metrics.update({
                    'connection_pool_usage': context.get('connection_pool_usage'),
                    'database_load': context.get('database_load'),
                    'concurrent_queries': context.get('concurrent_queries'),
                    'cache_hit_rate': context.get('cache_hit_rate'),
                    'lock_wait_time': context.get('lock_wait_time')
                })
            
            # ØªØ­Ù„ÙŠÙ„ Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
            metrics['performance_trend'] = self._analyze_performance_trend(query_text, execution_time)
            
            return metrics
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡: {e}")
            return {'error': str(e)}
    
    def _calculate_query_complexity_advanced(self, query_text: str) -> int:
        """Ø­Ø³Ø§Ø¨ ØªØ¹Ù‚ÙŠØ¯ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…ØªÙ‚Ø¯Ù…"""
        try:
            complexity_score = 0
            query_upper = query_text.upper()
            
            # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ù…Ø¹ Ø£ÙˆØ²Ø§Ù†
            complexity_factors = {
                'num_tables': (len(re.findall(r'\b(FROM|JOIN)\s+(\w+)', query_upper, re.IGNORECASE)), 2),
                'num_conditions': (len(re.findall(r'\b(WHERE|AND|OR|HAVING)\b', query_upper, re.IGNORECASE)), 1),
                'num_aggregates': (len(re.findall(r'\b(COUNT|SUM|AVG|MAX|MIN|GROUP_CONCAT)\s*\(', query_upper, re.IGNORECASE)), 2),
                'num_subqueries': (len(re.findall(r'\(\s*SELECT', query_upper, re.IGNORECASE)), 3),
                'num_joins': (len(re.findall(r'\b(INNER\s+JOIN|LEFT\s+JOIN|RIGHT\s+JOIN|FULL\s+JOIN)\b', query_upper, re.IGNORECASE)), 2),
                'num_group_by': (len(re.findall(r'\bGROUP BY\b', query_upper, re.IGNORECASE)), 2),
                'num_order_by': (len(re.findall(r'\bORDER BY\b', query_upper, re.IGNORECASE)), 1),
                'num_unions': (len(re.findall(r'\bUNION\b', query_upper, re.IGNORECASE)), 3),
                'query_length': (len(query_text) // 100, 0.5)  # ÙƒÙ„ 100 Ø­Ø±Ù
            }
            
            for factor, (value, weight) in complexity_factors.items():
                complexity_score += value * weight
            
            # Ø¹Ù‚ÙˆØ¨Ø§Øª Ù„Ø£Ù†Ù…Ø§Ø· Ù…Ø¹Ù‚Ø¯Ø©
            if 'DISTINCT' in query_upper and 'GROUP BY' in query_upper:
                complexity_score += 5  # ØªÙƒØ±Ø§Ø± ØºÙŠØ± Ø¶Ø±ÙˆØ±ÙŠ
            
            if 'LIKE' in query_upper and '%' in query_text:
                complexity_score += 3  # Ø¨Ø­Ø« Ù†Ù…Ø·
            
            return int(complexity_score)
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯: {e}")
            return 0
    
    def _estimate_index_usage_advanced(self, query_text: str) -> Dict[str, Any]:
        """ØªÙ‚Ø¯ÙŠØ± Ù…ØªÙ‚Ø¯Ù… Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙÙ‡Ø§Ø±Ø³"""
        try:
            parsed = sqlparse.parse(query_text)
            if not parsed:
                return {'error': 'Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…'}
            
            statement = parsed[0]
            index_analysis = {
                'potential_indexes': [],
                'suggested_indexes': [],
                'full_table_scan_risk': False,
                'index_merge_possible': False,
                'covering_index_possible': False,
                'missing_indexes': []
            }
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
            tables = self._extract_tables(statement)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø±ÙˆØ· WHERE
            where_conditions = self._extract_where_conditions(statement)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ø¹Ù…Ø¯Ø© JOIN
            join_conditions = self._extract_join_conditions(statement)
            
            # ØªØ­Ù„ÙŠÙ„ Ø´Ø±ÙˆØ· WHERE Ù„Ù„ÙÙ‡Ø§Ø±Ø³
            for table, conditions in where_conditions.items():
                for column, operator in conditions:
                    index_analysis['potential_indexes'].append({
                        'table': table,
                        'column': column,
                        'operator': operator,
                        'type': 'WHERE'
                    })
            
            # ØªØ­Ù„ÙŠÙ„ Ø´Ø±ÙˆØ· JOIN Ù„Ù„ÙÙ‡Ø§Ø±Ø³
            for table, conditions in join_conditions.items():
                for column, operator in conditions:
                    index_analysis['potential_indexes'].append({
                        'table': table,
                        'column': column,
                        'operator': operator,
                        'type': 'JOIN'
                    })
            
            # ØªØ­Ø¯ÙŠØ¯ Ø®Ø·Ø± Ø§Ù„Ù…Ø³Ø­ Ø§Ù„ÙƒØ§Ù…Ù„
            if not index_analysis['potential_indexes'] and 'WHERE' in query_text.upper():
                index_analysis['full_table_scan_risk'] = True
            
            # Ø§Ù‚ØªØ±Ø§Ø­ ÙÙ‡Ø§Ø±Ø³ ØªØºØ·ÙŠØ©
            select_columns = self._extract_select_columns(statement)
            for table, columns in select_columns.items():
                if len(columns) <= 5:  # Ø¹Ø¯Ø¯ Ø£Ø¹Ù…Ø¯Ø© Ù…Ø¹Ù‚ÙˆÙ„
                    index_analysis['covering_index_possible'] = True
                    index_analysis['suggested_indexes'].append({
                        'table': table,
                        'columns': columns,
                        'type': 'COVERING',
                        'reason': 'ÙŠÙ…ÙƒÙ† Ù„Ù„ÙÙ‡Ø±Ø³ ØªØºØ·ÙŠØ© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©'
                    })
            
            return index_analysis
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ‚Ø¯ÙŠØ± Ø§Ù„ÙÙ‡Ø§Ø±Ø³: {e}")
            return {'error': str(e)}
    
    def _extract_tables(self, statement) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…"""
        tables = []
        try:
            from_clause = None
            for token in statement.tokens:
                if token.is_keyword and token.value.upper() == 'FROM':
                    from_clause = token
                    break
            
            if from_clause:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø¨Ø¹Ø¯ FROM
                for token in statement.tokens[statement.tokens.index(from_clause) + 1:]:
                    if hasattr(token, 'value') and token.value.strip():
                        tables.append(token.value.strip())
                        break
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø¯Ø§ÙˆÙ„ JOIN
            for token in statement.flatten():
                if token.is_keyword and token.value.upper() in ['JOIN', 'INNER JOIN', 'LEFT JOIN', 'RIGHT JOIN']:
                    # Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªØ§Ù„ÙŠ Ù„Ù€ JOIN
                    join_index = statement.tokens.index(token)
                    if join_index + 1 < len(statement.tokens):
                        next_token = statement.tokens[join_index + 1]
                        if hasattr(next_token, 'value') and next_token.value.strip():
                            tables.append(next_token.value.strip())
            
            return list(set(tables))
        except:
            return []
    
    def _extract_where_conditions(self, statement) -> Dict[str, List[Tuple[str, str]]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø±ÙˆØ· WHERE"""
        conditions = {}
        try:
            where_found = False
            current_conditions = []
            
            for token in statement.flatten():
                if token.is_keyword and token.value.upper() == 'WHERE':
                    where_found = True
                    continue
                
                if where_found:
                    if token.is_keyword and token.value.upper() in ['GROUP', 'ORDER', 'LIMIT']:
                        break
                    
                    if hasattr(token, 'value'):
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙˆØ§Ù„Ù…Ø´ØºÙ„Ø§Øª
                        condition_text = token.value
                        column_matches = re.findall(r'(\w+)\s*([=<>!]+)', condition_text)
                        for column, operator in column_matches:
                            # Ø§ÙØªØ±Ø§Ø¶ Ø£Ù† Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ù‡Ùˆ Ø§Ù„Ø£ÙˆÙ„ (ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ† Ù‡Ø°Ø§)
                            table = self._extract_tables(statement)[0] if self._extract_tables(statement) else 'unknown'
                            if table not in conditions:
                                conditions[table] = []
                            conditions[table].append((column, operator))
            
            return conditions
        except:
            return {}
    
    def _extract_join_conditions(self, statement) -> Dict[str, List[Tuple[str, str]]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø±ÙˆØ· JOIN"""
        conditions = {}
        try:
            for token in statement.flatten():
                if token.is_keyword and token.value.upper() in ['JOIN', 'INNER JOIN', 'LEFT JOIN', 'RIGHT JOIN']:
                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ON Ø¨Ø¹Ø¯ JOIN
                    join_index = statement.tokens.index(token)
                    for i in range(join_index + 1, min(join_index + 10, len(statement.tokens))):
                        next_token = statement.tokens[i]
                        if next_token.is_keyword and next_token.value.upper() == 'ON':
                            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´Ø±ÙˆØ· Ø¨Ø¹Ø¯ ON
                            on_conditions = []
                            for j in range(i + 1, min(i + 10, len(statement.tokens))):
                                condition_token = statement.tokens[j]
                                if hasattr(condition_token, 'value'):
                                    condition_text = condition_token.value
                                    column_matches = re.findall(r'(\w+\.\w+)\s*=\s*(\w+\.\w+)', condition_text)
                                    for col1, col2 in column_matches:
                                        table1 = col1.split('.')[0]
                                        column1 = col1.split('.')[1]
                                        if table1 not in conditions:
                                            conditions[table1] = []
                                        conditions[table1].append((column1, '='))
                            break
            
            return conditions
        except:
            return {}
    
    def _extract_select_columns(self, statement) -> Dict[str, List[str]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ÙÙŠ SELECT"""
        columns = {}
        try:
            select_found = False
            current_columns = []
            
            for token in statement.flatten():
                if token.is_keyword and token.value.upper() == 'SELECT':
                    select_found = True
                    continue
                
                if select_found and token.is_keyword and token.value.upper() == 'FROM':
                    break
                
                if select_found and hasattr(token, 'value'):
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
                    column_text = token.value
                    column_matches = re.findall(r'\b(\w+(?:\.\w+)?)\b', column_text)
                    for column_match in column_matches:
                        if '.' in column_match:
                            table, column = column_match.split('.')
                            if table not in columns:
                                columns[table] = []
                            if column != '*':
                                columns[table].append(column)
            
            return columns
        except:
            return {}
    
    def _identify_advanced_bottlenecks(self, query_text: str) -> List[str]:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ø®ØªÙ†Ø§Ù‚Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©"""
        bottlenecks = []
        query_upper = query_text.upper()
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ÙƒÙ„ÙØ©
        expensive_patterns = [
            (r'LIKE\s+[\'"]%[^\'"]', "Ø§Ø³ØªØ®Ø¯Ø§Ù… LIKE Ù…Ø¹ Ø¨Ø¯Ø§ÙŠØ© % ÙŠÙ…Ù†Ø¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙÙ‡Ø§Ø±Ø³"),
            (r'SELECT \*', "Ø§Ø³ØªØ®Ø¯Ø§Ù… SELECT * ÙŠØ¬Ù„Ø¨ Ø£Ø¹Ù…Ø¯Ø© ØºÙŠØ± Ø¶Ø±ÙˆØ±ÙŠØ©"),
            (r'WHERE\s+[^=]+\([^)]+\)', "Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯ÙˆØ§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙÙŠ WHERE ÙŠÙ…Ù†Ø¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙÙ‡Ø§Ø±Ø³"),
            (r'JOIN.*ON.*\bOR\b', "Ø´Ø±ÙˆØ· OR ÙÙŠ JOIN Ù‚Ø¯ ØªÙ…Ù†Ø¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†"),
            (r'DISTINCT.*GROUP BY', "Ø§Ø³ØªØ®Ø¯Ø§Ù… DISTINCT Ù…Ø¹ GROUP BY Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø²Ø§Ø¦Ø¯Ø§Ù‹"),
            (r'HAVING.*WHERE', "Ø´Ø±ÙˆØ· HAVING ÙŠÙ…ÙƒÙ† Ù†Ù‚Ù„Ù‡Ø§ Ø¥Ù„Ù‰ WHERE"),
            (r'IN\s*\(\s*SELECT', "Ø§Ø³ØªØ®Ø¯Ø§Ù… IN Ù…Ø¹ Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙØ±Ø¹ÙŠ Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø¨Ø·ÙŠØ¦Ø§Ù‹"),
            (r'NOT IN', "Ø§Ø³ØªØ®Ø¯Ø§Ù… NOT IN Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø¨Ø·ÙŠØ¦Ø§Ù‹ Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª ÙƒØ¨ÙŠØ±Ø©"),
            (r'ORDER BY RAND\(\)', "Ø§Ø³ØªØ®Ø¯Ø§Ù… ORDER BY RAND() Ù…ÙƒÙ„Ù Ø¬Ø¯Ø§Ù‹")
        ]
        
        for pattern, message in expensive_patterns:
            if re.search(pattern, query_upper, re.IGNORECASE):
                bottlenecks.append(message)
        
        return bottlenecks
    
    def _estimate_memory_usage_advanced(self, query_text: str, rows_affected: int) -> Dict[str, float]:
        """ØªÙ‚Ø¯ÙŠØ± Ù…ØªÙ‚Ø¯Ù… Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        try:
            # ØªÙ‚Ø¯ÙŠØ± Ø£Ø³Ø§Ø³ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ
            base_memory = rows_affected * 1024  # 1KB Ù„ÙƒÙ„ ØµÙ
            
            # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„
            complexity = self._calculate_query_complexity_advanced(query_text)
            memory_multiplier = 1 + (complexity * 0.05)
            
            # ØªØ­Ù„ÙŠÙ„ Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            query_type = self._classify_query_advanced(query_text)
            type_multipliers = {
                QueryType.AGGREGATE: 1.5,
                QueryType.JOIN: 1.8,
                QueryType.COMPLEX_JOIN: 2.5,
                QueryType.SUBQUERY: 1.7,
                QueryType.UNION: 1.6,
                QueryType.WINDOW_FUNCTION: 1.4
            }
            
            type_multiplier = type_multipliers.get(query_type, 1.0)
            
            estimated_memory = base_memory * memory_multiplier * type_multiplier
            
            return {
                'estimated_memory_bytes': estimated_memory,
                'estimated_memory_mb': estimated_memory / 1024 / 1024,
                'complexity_factor': complexity,
                'type_multiplier': type_multiplier
            }
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {e}")
            return {'estimated_memory_bytes': 0, 'estimated_memory_mb': 0, 'complexity_factor': 0}
    
    def _estimate_io_operations(self, query_text: str, rows_affected: int) -> Dict[str, Any]:
        """ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„/Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬"""
        try:
            # ØªÙ‚Ø¯ÙŠØ± ØªÙ‚Ø±ÙŠØ¨ÙŠ Ù„Ø¹Ù…Ù„ÙŠØ§Øª I/O
            query_type = self._classify_query_advanced(query_text)
            
            io_estimates = {
                'estimated_reads': rows_affected,
                'estimated_writes': 0,
                'io_intensity': 'low'
            }
            
            if query_type in [QueryType.INSERT, QueryType.UPDATE, QueryType.DELETE]:
                io_estimates['estimated_writes'] = rows_affected
                io_estimates['io_intensity'] = 'medium'
            
            if rows_affected > 10000:
                io_estimates['io_intensity'] = 'high'
            
            return io_estimates
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ‚Ø¯ÙŠØ± I/O: {e}")
            return {'estimated_reads': 0, 'estimated_writes': 0, 'io_intensity': 'unknown'}
    
    def _estimate_execution_plan_quality(self, query_text: str) -> str:
        """ØªÙ‚Ø¯ÙŠØ± Ø¬ÙˆØ¯Ø© Ø®Ø·Ø© Ø§Ù„ØªÙ†ÙÙŠØ°"""
        try:
            score = 0
            query_upper = query_text.upper()
            
            # Ø¹ÙˆØ§Ù…Ù„ Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©
            if 'WHERE' in query_upper:
                score += 2
            
            if 'INDEX' in query_upper:
                score += 3
            
            if 'LIMIT' in query_upper:
                score += 1
            
            # Ø¹ÙˆØ§Ù…Ù„ Ø³Ù„Ø¨ÙŠØ©
            if 'SELECT *' in query_upper:
                score -= 2
            
            if 'LIKE \'%' in query_upper:
                score -= 3
            
            if 'ORDER BY RAND()' in query_upper:
                score -= 4
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬ÙˆØ¯Ø©
            if score >= 3:
                return 'excellent'
            elif score >= 0:
                return 'good'
            elif score >= -2:
                return 'fair'
            else:
                return 'poor'
                
        except:
            return 'unknown'
    
    def _analyze_performance_trend(self, query_text: str, current_time: float) -> str:
        """ØªØ­Ù„ÙŠÙ„ Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        try:
            query_hash = self._create_query_hash(query_text)
            pattern = self.query_patterns.get(query_hash)
            
            if not pattern or len(pattern.execution_history) < 3:
                return 'insufficient_data'
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡
            recent_times = pattern.execution_history[-5:]  # Ø¢Ø®Ø± 5 ØªÙ†ÙÙŠØ°Ø§Øª
            if len(recent_times) >= 3:
                trend = statistics.mean(recent_times[-3:]) - statistics.mean(recent_times[:3])
                
                if trend > current_time * 0.1:  # ØªØ¯Ù‡ÙˆØ± Ø¨Ø£ÙƒØ«Ø± Ù…Ù† 10%
                    return 'deteriorating'
                elif trend < -current_time * 0.1:  # ØªØ­Ø³Ù† Ø¨Ø£ÙƒØ«Ø± Ù…Ù† 10%
                    return 'improving'
                else:
                    return 'stable'
            
            return 'stable'
        except:
            return 'unknown'
    
    def _analyze_resource_usage(self, query_text: str, execution_time: float, rows_affected: int) -> Dict[str, float]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        try:
            memory_estimate = self._estimate_memory_usage_advanced(query_text, rows_affected)
            io_estimate = self._estimate_io_operations(query_text, rows_affected)
            
            return {
                'cpu_usage_estimate': execution_time * 1000,  # ØªÙ‚Ø¯ÙŠØ± Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ CPU
                'memory_usage_mb': memory_estimate.get('estimated_memory_mb', 0),
                'io_operations': io_estimate.get('estimated_reads', 0) + io_estimate.get('estimated_writes', 0),
                'execution_time': execution_time
            }
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯: {e}")
            return {'cpu_usage_estimate': 0, 'memory_usage_mb': 0, 'io_operations': 0, 'execution_time': execution_time}
    
    def _generate_advanced_optimization_suggestions(self, query_text: str, 
                                                  performance_metrics: Dict,
                                                  index_analysis: Dict,
                                                  query_type: QueryType,
                                                  query_complexity: int) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª ØªØ­Ø³ÙŠÙ† Ù…ØªÙ‚Ø¯Ù…Ø©"""
        suggestions = []
        
        # Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        type_suggestions = self._get_query_type_suggestions(query_type, query_text)
        suggestions.extend(type_suggestions)
        
        # Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø§Ù„ÙÙ‡Ø§Ø±Ø³
        if self.optimization_rules['index_optimization']['enabled']:
            index_suggestions = self._suggest_advanced_indexes(query_text, index_analysis, performance_metrics)
            suggestions.extend(index_suggestions)
        
        # Ø¥Ø¹Ø§Ø¯Ø© Ù‡ÙŠÙƒÙ„Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        if self.optimization_rules['query_restructuring']['enabled']:
            restructuring_suggestions = self._suggest_advanced_restructuring(query_text, query_type, query_complexity)
            suggestions.extend(restructuring_suggestions)
        
        # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        if self.optimization_rules['caching_strategy']['enabled']:
            caching_suggestions = self._suggest_advanced_caching(query_text, performance_metrics, query_type)
            suggestions.extend(caching_suggestions)
        
        # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
        if self.optimization_rules['resource_optimization']['enabled']:
            resource_suggestions = self._suggest_resource_optimizations(query_text, performance_metrics)
            suggestions.extend(resource_suggestions)
        
        return suggestions[:15]  # Ø¥Ø±Ø¬Ø§Ø¹ Ø£ÙˆÙ„ 15 Ø§Ù‚ØªØ±Ø§Ø­ ÙÙ‚Ø·
    
    def _get_query_type_suggestions(self, query_type: QueryType, query_text: str) -> List[str]:
        """Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù…Ø®ØµØµØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…"""
        suggestions = []
        query_upper = query_text.upper()
        
        if query_type == QueryType.JOIN:
            if 'LEFT JOIN' in query_upper and 'IS NULL' in query_upper:
                suggestions.append("ÙÙƒØ± ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… NOT EXISTS Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† LEFT JOIN ... IS NULL")
            
            if query_upper.count('JOIN') > 3:
                suggestions.append("Ø¹Ø¯Ø¯ JOINs ÙƒØ¨ÙŠØ± - ÙÙƒØ± ÙÙŠ Ø¥Ø¹Ø§Ø¯Ø© ØªØµÙ…ÙŠÙ… Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ÙØ±Ø¹ÙŠØ©")
        
        elif query_type == QueryType.SUBQUERY:
            if 'IN' in query_upper and '(SELECT' in query_upper:
                suggestions.append("ÙÙƒØ± ÙÙŠ ØªØ­ÙˆÙŠÙ„ IN (SELECT) Ø¥Ù„Ù‰ EXISTS Ø£Ùˆ JOIN")
        
        elif query_type == QueryType.AGGREGATE:
            if 'DISTINCT' in query_upper and 'GROUP BY' in query_upper:
                suggestions.append("Ø§Ø³ØªØ®Ø¯Ø§Ù… DISTINCT Ù…Ø¹ GROUP BY Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø²Ø§Ø¦Ø¯Ø§Ù‹ Ø¹Ù† Ø§Ù„Ø­Ø§Ø¬Ø©")
        
        elif query_type == QueryType.UNION:
            suggestions.append("ÙÙƒØ± ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… UNION ALL Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† UNION Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù‡Ù†Ø§Ùƒ ØªÙƒØ±Ø§Ø±Ø§Øª")
        
        return suggestions
    
    def _suggest_advanced_indexes(self, query_text: str, index_analysis: Dict, 
                                performance_metrics: Dict) -> List[str]:
        """Ø§Ù‚ØªØ±Ø§Ø­ ÙÙ‡Ø§Ø±Ø³ Ù…ØªÙ‚Ø¯Ù…Ø©"""
        suggestions = []
        
        if index_analysis.get('full_table_scan_risk'):
            suggestions.append("ğŸ”´ Ø®Ø·Ø± Ø§Ù„Ù…Ø³Ø­ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ø¬Ø¯ÙˆÙ„ - Ø£Ø¶Ù ÙÙ‡Ø§Ø±Ø³ Ø¹Ù„Ù‰ Ø£Ø¹Ù…Ø¯Ø© WHERE")
        
        for suggested_index in index_analysis.get('suggested_indexes', []):
            if suggested_index['type'] == 'COVERING':
                suggestions.append(f"ğŸŸ¢ ÙÙ‡Ø±Ø³ ØªØºØ·ÙŠØ© Ù…Ù‚ØªØ±Ø­ Ø¹Ù„Ù‰ {suggested_index['table']}.{suggested_index['columns']}")
            else:
                suggestions.append(f"ğŸŸ¡ ÙÙ‡Ø±Ø³ Ù…Ù‚ØªØ±Ø­ Ø¹Ù„Ù‰ {suggested_index['table']}.{suggested_index['column']}")
        
        # Ø§Ù‚ØªØ±Ø§Ø­ ÙÙ‡Ø§Ø±Ø³ Ù…Ø±ÙƒØ¨Ø©
        where_conditions = []
        for condition in index_analysis.get('potential_indexes', []):
            if condition['type'] == 'WHERE':
                where_conditions.append(condition)
        
        if len(where_conditions) >= 2:
            tables = {}
            for condition in where_conditions:
                table = condition['table']
                if table not in tables:
                    tables[table] = []
                tables[table].append(condition['column'])
            
            for table, columns in tables.items():
                if len(columns) >= 2:
                    suggestions.append(f"ğŸŸ  ÙÙ‡Ø±Ø³ Ù…Ø±ÙƒØ¨ Ù…Ù‚ØªØ±Ø­ Ø¹Ù„Ù‰ {table} ({', '.join(columns[:3])})")
        
        return suggestions
    
    def _suggest_advanced_restructuring(self, query_text: str, query_type: QueryType, 
                                      query_complexity: int) -> List[str]:
        """Ø§Ù‚ØªØ±Ø§Ø­ Ø¥Ø¹Ø§Ø¯Ø© Ù‡ÙŠÙƒÙ„Ø© Ù…ØªÙ‚Ø¯Ù…Ø©"""
        suggestions = []
        query_upper = query_text.upper()
        
        if query_complexity > self.performance_thresholds['complexity_threshold']:
            suggestions.append("ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ø±ØªÙØ¹ - ÙÙƒØ± ÙÙŠ ØªÙ‚Ø³ÙŠÙ…Ù‡ Ø¥Ù„Ù‰ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø£ØµØºØ±")
        
        if self.optimization_rules['query_restructuring']['enable_subquery_to_join']:
            if query_type == QueryType.SUBQUERY and 'IN' in query_upper:
                suggestions.append("ÙŠÙ…ÙƒÙ† ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„ÙØ±Ø¹ÙŠ IN Ø¥Ù„Ù‰ JOIN Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡")
        
        if self.optimization_rules['query_restructuring']['enable_predicate_pushdown']:
            if 'HAVING' in query_upper and 'WHERE' in query_upper:
                suggestions.append("ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠÙ…ÙƒÙ† Ù†Ù‚Ù„ Ø¨Ø¹Ø¶ Ø´Ø±ÙˆØ· HAVING Ø¥Ù„Ù‰ WHERE")
        
        if 'ORDER BY' in query_upper and 'LIMIT' not in query_upper:
            suggestions.append("Ø¥Ø¶Ø§ÙØ© LIMIT Ø¥Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ø¹ ORDER BY ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØ­Ø³Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡")
        
        return suggestions
    
    def _suggest_advanced_caching(self, query_text: str, performance_metrics: Dict, 
                                query_type: QueryType) -> List[str]:
        """Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ù…ØªÙ‚Ø¯Ù…Ø©"""
        suggestions = []
        
        if performance_metrics['is_slow_query']:
            suggestions.append("Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø·ÙŠØ¡ - Ù…Ø«Ø§Ù„ÙŠ Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª")
        
        if query_type in [QueryType.SELECT, QueryType.AGGREGATE]:
            if performance_metrics.get('rows_affected', 0) < self.optimization_rules['caching_strategy']['result_size_threshold']:
                suggestions.append("Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ØµØºÙŠØ±Ø© - Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª")
        
        if performance_metrics.get('execution_plan_quality') == 'poor':
            suggestions.append("Ø®Ø·Ø© Ø§Ù„ØªÙ†ÙÙŠØ° Ø¶Ø¹ÙŠÙØ© - Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØ­Ø³Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡")
        
        return suggestions
    
    def _suggest_resource_optimizations(self, query_text: str, performance_metrics: Dict) -> List[str]:
        """Ø§Ù‚ØªØ±Ø§Ø­ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        suggestions = []
        
        memory_usage = performance_metrics.get('resource_usage', {}).get('memory_usage_mb', 0)
        if memory_usage > self.optimization_rules['resource_optimization']['memory_usage_threshold']:
            suggestions.append(f"Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ø±ØªÙØ¹ ({memory_usage:.1f}MB) - ÙÙƒØ± ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…")
        
        if performance_metrics.get('io_estimate', {}).get('io_intensity') == 'high':
            suggestions.append("ÙƒØ«Ø§ÙØ© I/O Ø¹Ø§Ù„ÙŠØ© - ÙÙƒØ± ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø£Ùˆ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        
        if self.optimization_rules['resource_optimization']['enable_batch_processing']:
            if performance_metrics.get('rows_affected', 0) > 1000:
                suggestions.append("Ø¹Ø¯Ø¯ Ø§Ù„ØµÙ ÙƒØ¨ÙŠØ± - ÙÙƒØ± ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹ÙŠØ©")
        
        return suggestions
    
    def _determine_optimization_level_advanced(self, execution_time: float, 
                                             performance_metrics: Dict, 
                                             suggestion_count: int,
                                             query_complexity: int) -> OptimizationLevel:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ ØªØ­Ø³ÙŠÙ† Ù…ØªÙ‚Ø¯Ù…"""
        # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„ØªØ±Ø¬ÙŠØ­
        time_factor = 0.4
        complexity_factor = 0.3
        suggestion_factor = 0.3
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ÙˆÙ‚Øª
        time_score = 0
        if execution_time > self.performance_thresholds['critical_query_threshold']:
            time_score = 1.0
        elif execution_time > self.performance_thresholds['slow_query_threshold']:
            time_score = 0.7
        elif execution_time > self.performance_thresholds['slow_query_threshold'] / 2:
            time_score = 0.3
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        complexity_score = 0
        if query_complexity > self.performance_thresholds['complexity_threshold'] * 1.5:
            complexity_score = 1.0
        elif query_complexity > self.performance_thresholds['complexity_threshold']:
            complexity_score = 0.7
        elif query_complexity > self.performance_thresholds['complexity_threshold'] / 2:
            complexity_score = 0.3
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª
        suggestion_score = min(suggestion_count / 10.0, 1.0)
        
        # Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
        total_score = (time_score * time_factor + 
                      complexity_score * complexity_factor + 
                      suggestion_score * suggestion_factor)
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³ØªÙˆÙ‰
        if total_score >= 0.8:
            return OptimizationLevel.EMERGENCY
        elif total_score >= 0.6:
            return OptimizationLevel.CRITICAL
        elif total_score >= 0.4:
            return OptimizationLevel.HIGH
        elif total_score >= 0.2:
            return OptimizationLevel.MEDIUM
        elif total_score > 0:
            return OptimizationLevel.LOW
        else:
            return OptimizationLevel.OPTIMAL
    
    def _create_query_hash(self, query_text: str) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø§Ø´ ÙØ±ÙŠØ¯ Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…"""
        try:
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            normalized = re.sub(r'\s+', ' ', query_text.upper().strip())
            normalized = re.sub(r'\d+', '?', normalized)
            normalized = re.sub(r"'[^']*'", '?', normalized)
            normalized = re.sub(r'"[^"]*"', '?', normalized)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‡Ø§Ø´
            return hashlib.md5(normalized.encode('utf-8')).hexdigest()
        except:
            return hashlib.md5(query_text.encode('utf-8')).hexdigest()
    
    def _update_query_history(self, analysis: QueryAnalysis):
        """ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª"""
        self.query_history.append(analysis)
        
        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø¢Ø®Ø± 5000 Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙ‚Ø·
        if len(self.query_history) > 5000:
            self.query_history = self.query_history[-5000:]
    
    def _update_query_patterns(self, analysis: QueryAnalysis):
        """ØªØ­Ø¯ÙŠØ« Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        try:
            query_hash = self._create_query_hash(analysis.query_text)
            
            if query_hash not in self.query_patterns:
                self.query_patterns[query_hash] = QueryPattern(
                    pattern_hash=query_hash,
                    normalized_query=re.sub(r'\s+', ' ', analysis.query_text.upper().strip()),
                    frequency=1,
                    avg_execution_time=analysis.execution_time,
                    max_execution_time=analysis.execution_time,
                    min_execution_time=analysis.execution_time,
                    optimization_opportunities=analysis.optimization_suggestions,
                    last_executed=analysis.timestamp,
                    execution_history=[analysis.execution_time]
                )
            else:
                pattern = self.query_patterns[query_hash]
                pattern.frequency += 1
                pattern.execution_history.append(analysis.execution_time)
                
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
                pattern.avg_execution_time = statistics.mean(pattern.execution_history)
                pattern.max_execution_time = max(pattern.execution_history)
                pattern.min_execution_time = min(pattern.execution_history)
                pattern.last_executed = analysis.timestamp
                
                # ØªØ­Ø¯ÙŠØ« ÙØ±Øµ Ø§Ù„ØªØ­Ø³ÙŠÙ†
                for suggestion in analysis.optimization_suggestions:
                    if suggestion not in pattern.optimization_opportunities:
                        pattern.optimization_opportunities.append(suggestion)
                
                # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø¢Ø®Ø± 50 ØªÙ†ÙÙŠØ° ÙÙ‚Ø·
                if len(pattern.execution_history) > 50:
                    pattern.execution_history = pattern.execution_history[-50:]
                    
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£Ù†Ù…Ø§Ø·: {e}")
    
    def _update_index_recommendations(self, analysis: QueryAnalysis, index_analysis: Dict):
        """ØªØ­Ø¯ÙŠØ« ØªÙˆØµÙŠØ§Øª Ø§Ù„ÙÙ‡Ø§Ø±Ø³"""
        try:
            for suggested_index in index_analysis.get('suggested_indexes', []):
                # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙØ§Ø¦Ø¯Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©
                expected_benefit = self._calculate_index_benefit(analysis, suggested_index)
                
                if expected_benefit > self.optimization_rules['index_optimization']['min_benefit']:
                    recommendation = IndexRecommendation(
                        table_name=suggested_index.get('table', 'unknown'),
                        column_names=suggested_index.get('columns', [suggested_index.get('column', 'unknown')]),
                        index_type=IndexType.BTREE,
                        expected_benefit=expected_benefit,
                        creation_cost=self._estimate_index_creation_cost(suggested_index),
                        priority=self._calculate_index_priority(expected_benefit, analysis),
                        reason=suggested_index.get('reason', 'ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡'),
                        sql_statement=self._generate_index_sql(suggested_index)
                    )
                    
                    # Ø¥Ø¶Ø§ÙØ© Ø£Ùˆ ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙˆØµÙŠØ©
                    self._add_or_update_index_recommendation(recommendation)
                    
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¯ÙŠØ« ØªÙˆØµÙŠØ§Øª Ø§Ù„ÙÙ‡Ø§Ø±Ø³: {e}")
    
    def _calculate_index_benefit(self, analysis: QueryAnalysis, suggested_index: Dict) -> float:
        """Ø­Ø³Ø§Ø¨ ÙØ§Ø¦Ø¯Ø© Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©"""
        try:
            base_score = 0.0
            
            # ÙØ§Ø¦Ø¯Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            query_type_benefits = {
                QueryType.SELECT: 0.7,
                QueryType.JOIN: 0.8,
                QueryType.WHERE: 0.9,
                QueryType.ORDER_BY: 0.6
            }
            
            base_score = query_type_benefits.get(analysis.query_type, 0.5)
            
            # ØªØ¹Ø¯ÙŠÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙˆÙ‚Øª Ø§Ù„ØªÙ†ÙÙŠØ°
            if analysis.execution_time > self.performance_thresholds['slow_query_threshold']:
                base_score *= 1.5
            
            # ØªØ¹Ø¯ÙŠÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙƒØ±Ø§Ø±
            query_hash = self._create_query_hash(analysis.query_text)
            pattern = self.query_patterns.get(query_hash)
            if pattern and pattern.frequency > 10:
                base_score *= 1.3
            
            return min(base_score, 1.0)
            
        except:
            return 0.5
    
    def _estimate_index_creation_cost(self, suggested_index: Dict) -> float:
        """ØªÙ‚Ø¯ÙŠØ± ØªÙƒÙ„ÙØ© Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³"""
        try:
            base_cost = 1.0
            num_columns = len(suggested_index.get('columns', [1]))
            
            # ØªÙƒÙ„ÙØ© Ø£Ø¹Ù„Ù‰ Ù„Ù„ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ù…Ø±ÙƒØ¨Ø©
            if num_columns > 1:
                base_cost *= (1 + (num_columns * 0.2))
            
            return base_cost
        except:
            return 1.0
    
    def _calculate_index_priority(self, expected_benefit: float, analysis: QueryAnalysis) -> int:
        """Ø­Ø³Ø§Ø¨ Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„ÙÙ‡Ø±Ø³"""
        try:
            priority = int(expected_benefit * 100)
            
            # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø¨Ø·ÙŠØ¦Ø©
            if analysis.execution_time > self.performance_thresholds['slow_query_threshold']:
                priority += 20
            
            # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©
            query_hash = self._create_query_hash(analysis.query_text)
            pattern = self.query_patterns.get(query_hash)
            if pattern and pattern.frequency > 5:
                priority += 15
            
            return min(priority, 100)
        except:
            return 50
    
    def _generate_index_sql(self, suggested_index: Dict) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ø¨Ø§Ø±Ø© SQL Ù„Ù„ÙÙ‡Ø±Ø³"""
        try:
            table_name = suggested_index.get('table', 'table_name')
            columns = suggested_index.get('columns', ['column_name'])
            index_name = f"idx_{table_name}_{'_'.join(columns)}"
            
            return f"CREATE INDEX {index_name} ON {table_name} ({', '.join(columns)});"
        except:
            return "CREATE INDEX idx_name ON table_name (column);"
    
    def _add_or_update_index_recommendation(self, new_recommendation: IndexRecommendation):
        """Ø¥Ø¶Ø§ÙØ© Ø£Ùˆ ØªØ­Ø¯ÙŠØ« ØªÙˆØµÙŠØ© ÙÙ‡Ø±Ø³"""
        try:
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØªÙˆØµÙŠØ© Ù…ÙˆØ¬ÙˆØ¯Ø©
            existing_index = None
            for i, rec in enumerate(self.index_recommendations):
                if (rec.table_name == new_recommendation.table_name and 
                    rec.column_names == new_recommendation.column_names):
                    existing_index = i
                    break
            
            if existing_index is not None:
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙˆØµÙŠØ© Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©
                existing_rec = self.index_recommendations[existing_index]
                if new_recommendation.expected_benefit > existing_rec.expected_benefit:
                    self.index_recommendations[existing_index] = new_recommendation
            else:
                # Ø¥Ø¶Ø§ÙØ© ØªÙˆØµÙŠØ© Ø¬Ø¯ÙŠØ¯Ø©
                self.index_recommendations.append(new_recommendation)
                
            # ØªØ±ØªÙŠØ¨ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø­Ø³Ø¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©
            self.index_recommendations.sort(key=lambda x: x.priority, reverse=True)
            
            # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø£Ø¹Ù„Ù‰ 50 ØªÙˆØµÙŠØ© ÙÙ‚Ø·
            if len(self.index_recommendations) > 50:
                self.index_recommendations = self.index_recommendations[:50]
                
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¶Ø§ÙØ© ØªÙˆØµÙŠØ© Ø§Ù„ÙÙ‡Ø±Ø³: {e}")
    
    def _create_error_analysis(self, query_text: str, error: str) -> QueryAnalysis:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø®Ø·Ø£"""
        return QueryAnalysis(
            query_id=f"error_{int(time.time())}",
            query_text=query_text,
            query_type=QueryType.SELECT,
            execution_time=0,
            rows_affected=0,
            optimization_suggestions=[f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {error}"],
            optimization_level=OptimizationLevel.CRITICAL,
            performance_metrics={'error': error},
            query_complexity=0,
            index_analysis={},
            resource_usage={}
        )
    
    def get_comprehensive_performance_report(self, hours: int = 24) -> PerformanceReport:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙ‚Ø±ÙŠØ± Ø£Ø¯Ø§Ø¡ Ø´Ø§Ù…Ù„"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            recent_queries = [q for q in self.query_history if q.timestamp > cutoff_time]
            
            if not recent_queries:
                return PerformanceReport(
                    period_start=cutoff_time,
                    period_end=datetime.now(),
                    total_queries=0,
                    slow_queries=0,
                    avg_execution_time=0,
                    performance_distribution={},
                    top_slow_queries=[],
                    index_recommendations=[],
                    system_recommendations=["Ù„Ø§ ØªÙˆØ¬Ø¯ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø­Ø¯ÙŠØ«Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„"],
                    resource_utilization={}
                )
            
            # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            execution_times = [q.execution_time for q in recent_queries]
            slow_queries = [q for q in recent_queries 
                          if q.execution_time > self.performance_thresholds['slow_query_threshold']]
            
            # ØªÙˆØ²ÙŠØ¹ Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†
            performance_distribution = {}
            for level in OptimizationLevel:
                performance_distribution[level] = len(
                    [q for q in recent_queries if q.optimization_level == level]
                )
            
            # Ø£Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø¨Ø·ÙŠØ¦Ø©
            top_slow_queries = sorted(
                [q for q in recent_queries if q.optimization_level in [OptimizationLevel.CRITICAL, OptimizationLevel.EMERGENCY]],
                key=lambda x: x.execution_time,
                reverse=True
            )[:10]
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
            total_memory = sum(q.resource_usage.get('memory_usage_mb', 0) for q in recent_queries)
            total_cpu = sum(q.resource_usage.get('cpu_usage_estimate', 0) for q in recent_queries)
            
            resource_utilization = {
                'avg_memory_usage_mb': total_memory / len(recent_queries) if recent_queries else 0,
                'avg_cpu_usage': total_cpu / len(recent_queries) if recent_queries else 0,
                'total_queries': len(recent_queries),
                'peak_memory_usage_mb': max([q.resource_usage.get('memory_usage_mb', 0) for q in recent_queries]) if recent_queries else 0
            }
            
            return PerformanceReport(
                period_start=cutoff_time,
                period_end=datetime.now(),
                total_queries=len(recent_queries),
                slow_queries=len(slow_queries),
                avg_execution_time=statistics.mean(execution_times) if execution_times else 0,
                performance_distribution=performance_distribution,
                top_slow_queries=top_slow_queries,
                index_recommendations=self.index_recommendations[:10],  # Ø£Ù‡Ù… 10 ØªÙˆØµÙŠØ§Øª
                system_recommendations=self._generate_system_recommendations(recent_queries),
                resource_utilization=resource_utilization
            )
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡: {e}")
            return PerformanceReport(
                period_start=datetime.now() - timedelta(hours=hours),
                period_end=datetime.now(),
                total_queries=0,
                slow_queries=0,
                avg_execution_time=0,
                performance_distribution={},
                top_slow_queries=[],
                index_recommendations=[],
                system_recommendations=[f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {str(e)}"],
                resource_utilization={}
            )
    
    def _generate_system_recommendations(self, queries: List[QueryAnalysis]) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…"""
        recommendations = []
        
        if not queries:
            return ["Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª"]
        
        # Ù†Ø³Ø¨Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø¨Ø·ÙŠØ¦Ø©
        slow_queries = [q for q in queries if q.execution_time > self.performance_thresholds['slow_query_threshold']]
        slow_percentage = (len(slow_queries) / len(queries)) * 100
        
        if slow_percentage > 30:
            recommendations.append(f"ğŸš¨ Ù†Ø³Ø¨Ø© Ø¹Ø§Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø¨Ø·ÙŠØ¦Ø© ({slow_percentage:.1f}%) - Ù…Ø±Ø§Ø¬Ø¹Ø© Ø¹Ø§Ø¬Ù„Ø© Ù…Ø·Ù„ÙˆØ¨Ø©")
        elif slow_percentage > 15:
            recommendations.append(f"âš ï¸  Ù†Ø³Ø¨Ø© Ù…ØªÙˆØ³Ø·Ø© Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø¨Ø·ÙŠØ¦Ø© ({slow_percentage:.1f}%) - ØªØ­Ø³ÙŠÙ†Ø§Øª Ù…Ø³ØªØ­Ø¨Ø©")
        
        # ØªÙˆØ²ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
        query_types = {}
        for query in queries:
            query_types[query.query_type] = query_types.get(query.query_type, 0) + 1
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        avg_complexity = statistics.mean([q.query_complexity for q in queries])
        if avg_complexity > 15:
            recommendations.append("ğŸ“Š Ù…ØªÙˆØ³Ø· ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ø±ØªÙØ¹ - ÙÙƒØ± ÙÙŠ ØªØ¨Ø³ÙŠØ· Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        avg_memory = statistics.mean([q.resource_usage.get('memory_usage_mb', 0) for q in queries])
        if avg_memory > 50:
            recommendations.append(f"ğŸ§  Ù…ØªÙˆØ³Ø· Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ø±ØªÙØ¹ ({avg_memory:.1f}MB) - ÙÙƒØ± ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø©")
        
        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø·
        pattern_recommendations = self._generate_pattern_based_recommendations()
        recommendations.extend(pattern_recommendations)
        
        return recommendations[:10]  # Ø¥Ø±Ø¬Ø§Ø¹ Ø£ÙˆÙ„ 10 ØªÙˆØµÙŠØ§Øª ÙÙ‚Ø·
    
    def _generate_pattern_based_recommendations(self) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø·"""
        recommendations = []
        
        try:
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø­Ø±Ø¬Ø©
            critical_patterns = []
            for pattern in self.query_patterns.values():
                if (pattern.avg_execution_time > self.performance_thresholds['critical_query_threshold'] and 
                    pattern.frequency > 5):
                    critical_patterns.append(pattern)
            
            if critical_patterns:
                recommendations.append(f"ğŸ” ØªÙ… Ø§ÙƒØªØ´Ø§Ù {len(critical_patterns)} Ù†Ù…Ø· Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø­Ø±Ø¬ ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ† Ø¹Ø§Ø¬Ù„")
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©
            frequent_patterns = []
            for pattern in self.query_patterns.values():
                if pattern.frequency > 20:
                    frequent_patterns.append(pattern)
            
            if frequent_patterns:
                recommendations.append(f"ğŸ”„ ØªÙ… Ø§ÙƒØªØ´Ø§Ù {len(frequent_patterns)} Ù†Ù…Ø· Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…ØªÙƒØ±Ø± - Ù…Ø«Ø§Ù„ÙŠ Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª")
            
            return recommendations
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø£Ù†Ù…Ø§Ø·: {e}")
            return []
    
    def get_query_patterns_report(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙ‚Ø±ÙŠØ± Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª"""
        try:
            critical_patterns = []
            frequent_patterns = []
            optimized_patterns = []
            
            for pattern in self.query_patterns.values():
                if pattern.avg_execution_time > self.performance_thresholds['critical_query_threshold']:
                    critical_patterns.append(pattern)
                elif pattern.frequency > self.performance_thresholds['frequent_query_threshold']:
                    frequent_patterns.append(pattern)
                elif pattern.avg_execution_time < self.performance_thresholds['slow_query_threshold'] / 2:
                    optimized_patterns.append(pattern)
            
            return {
                'total_patterns': len(self.query_patterns),
                'critical_patterns_count': len(critical_patterns),
                'frequent_patterns_count': len(frequent_patterns),
                'optimized_patterns_count': len(optimized_patterns),
                'critical_patterns': [
                    {
                        'pattern': p.normalized_query[:100] + '...' if len(p.normalized_query) > 100 else p.normalized_query,
                        'frequency': p.frequency,
                        'avg_execution_time': p.avg_execution_time,
                        'optimization_opportunities': p.optimization_opportunities[:3]
                    }
                    for p in critical_patterns[:5]  # Ø£ÙˆÙ„ 5 Ø£Ù†Ù…Ø§Ø· Ø­Ø±Ø¬Ø© ÙÙ‚Ø·
                ],
                'frequent_patterns': [
                    {
                        'pattern': p.normalized_query[:100] + '...' if len(p.normalized_query) > 100 else p.normalized_query,
                        'frequency': p.frequency,
                        'avg_execution_time': p.avg_execution_time
                    }
                    for p in frequent_patterns[:5]  # Ø£ÙˆÙ„ 5 Ø£Ù†Ù…Ø§Ø· Ù…ØªÙƒØ±Ø±Ø© ÙÙ‚Ø·
                ]
            }
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ù†Ù…Ø§Ø·: {e}")
            return {'error': str(e)}
    
    def optimize_query_execution(self, query_text: str, context: Dict = None) -> Dict[str, Any]:
        """ØªØ­Ø³ÙŠÙ† ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ø¹ Ø¥Ø±Ø¬Ø§Ø¹ Ø®Ø·Ø© Ø§Ù„ØªØ­Ø³ÙŠÙ†"""
        try:
            # Ù…Ø­Ø§ÙƒØ§Ø© ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            start_time = time.time()
            
            # Ù‡Ù†Ø§ Ø³ÙŠØªÙ… ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„ÙØ¹Ù„ÙŠ ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ
            # Ù„Ù„Ù…Ø«Ø§Ù„ØŒ Ø³Ù†Ø³ØªØ®Ø¯Ù… ÙˆÙ‚Øª ØªÙ†ÙÙŠØ° ÙˆÙ‡Ù…ÙŠ
            execution_time = 0.1  # ÙˆÙ‚Øª ØªÙ†ÙÙŠØ° Ø§ÙØªØ±Ø§Ø¶ÙŠ
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            analysis = self.analyze_query(query_text, execution_time, 0, context)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø·Ø© Ø§Ù„ØªØ­Ø³ÙŠÙ†
            optimization_plan = {
                'original_query': query_text,
                'analysis': analysis,
                'optimization_strategies': [],
                'expected_improvement': 0.0,
                'execution_plan': self._generate_execution_plan(query_text, analysis)
            }
            
            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†
            if analysis.optimization_suggestions:
                optimization_plan['optimization_strategies'] = analysis.optimization_suggestions[:5]
                optimization_plan['expected_improvement'] = self._estimate_improvement(analysis)
            
            optimization_plan['total_analysis_time'] = time.time() - start_time
            
            return optimization_plan
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø³ÙŠÙ† ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {e}")
            return {'error': str(e)}
    
    def _generate_execution_plan(self, query_text: str, analysis: QueryAnalysis) -> Dict[str, Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø·Ø© ØªÙ†ÙÙŠØ° Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…"""
        try:
            return {
                'query_type': analysis.query_type.value,
                'estimated_rows': analysis.rows_affected,
                'estimated_cost': analysis.execution_time * 1000,  # ØªÙ‚Ø¯ÙŠØ± Ø§Ù„ØªÙƒÙ„ÙØ©
                'recommended_indexes': analysis.index_analysis.get('suggested_indexes', [])[:3],
                'bottlenecks': analysis.performance_metrics.get('potential_bottlenecks', [])[:3],
                'memory_requirements': analysis.resource_usage.get('memory_usage_mb', 0),
                'io_operations': analysis.performance_metrics.get('io_estimate', {}).get('estimated_reads', 0)
            }
        except:
            return {}
    
    def _estimate_improvement(self, analysis: QueryAnalysis) -> float:
        """ØªÙ‚Ø¯ÙŠØ± Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©"""
        try:
            base_improvement = 0.0
            
            # ØªØ­Ø³ÙŠÙ† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ­Ø³ÙŠÙ†
            improvement_factors = {
                OptimizationLevel.EMERGENCY: 0.7,
                OptimizationLevel.CRITICAL: 0.5,
                OptimizationLevel.HIGH: 0.3,
                OptimizationLevel.MEDIUM: 0.2,
                OptimizationLevel.LOW: 0.1,
                OptimizationLevel.OPTIMAL: 0.0
            }
            
            base_improvement = improvement_factors.get(analysis.optimization_level, 0.0)
            
            # ØªØ¹Ø¯ÙŠÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª
            suggestion_factor = min(len(analysis.optimization_suggestions) / 10.0, 1.0)
            base_improvement *= (1 + suggestion_factor * 0.5)
            
            return min(base_improvement, 0.9)  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 90% ØªØ­Ø³Ù†
            
        except:
            return 0.0
    
    def export_optimization_data(self, file_path: str) -> bool:
        """ØªØµØ¯ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø¥Ù„Ù‰ Ù…Ù„Ù"""
        try:
            export_data = {
                'export_timestamp': datetime.now().isoformat(),
                'performance_thresholds': self.performance_thresholds,
                'optimization_rules': self.optimization_rules,
                'query_patterns_count': len(self.query_patterns),
                'index_recommendations_count': len(self.index_recommendations),
                'recent_performance_report': self.get_comprehensive_performance_report(24).__dict__,
                'top_index_recommendations': [
                    {
                        'table': rec.table_name,
                        'columns': rec.column_names,
                        'priority': rec.priority,
                        'expected_benefit': rec.expected_benefit,
                        'sql': rec.sql_statement
                    }
                    for rec in self.index_recommendations[:10]
                ]
            }
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"âœ… ØªÙ… ØªØµØ¯ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø¥Ù„Ù‰ {file_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return False
    
    def import_optimization_data(self, file_path: str) -> bool:
        """Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ† Ù…Ù† Ù…Ù„Ù"""
        try:
            if not os.path.exists(file_path):
                logger.error(f"âŒ Ù…Ù„Ù Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {file_path}")
                return False
            
            with open(file_path, 'r', encoding='utf-8') as f:
                import_data = json.load(f)
            
            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªÙˆØ±Ø¯Ø© (ÙŠÙ…ÙƒÙ† ØªØ®ØµÙŠØµ Ù‡Ø°Ø§ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©)
            if 'performance_thresholds' in import_data:
                self.performance_thresholds.update(import_data['performance_thresholds'])
            
            if 'optimization_rules' in import_data:
                self.optimization_rules.update(import_data['optimization_rules'])
            
            logger.info(f"âœ… ØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ† Ù…Ù† {file_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return False

class QueryPerformanceMonitor:
    """Ù…Ø±Ø§Ù‚Ø¨ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª"""
    
    def __init__(self):
        self.is_monitoring = False
        self.monitor_thread = None
    
    def start_monitoring(self):
        """Ø¨Ø¯Ø¡ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        if self.is_monitoring:
            return
        
        self.is_monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitor_thread.start()
        logger.info("ğŸ” Ø¨Ø¯Ø¡ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª")
    
    def stop_monitoring(self):
        """Ø¥ÙŠÙ‚Ø§Ù Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        self.is_monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5.0)
        logger.info("ğŸ›‘ Ø¥ÙŠÙ‚Ø§Ù Ù…Ø±Ø§Ù‚Ø¨Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª")
    
    def _monitoring_loop(self):
        """Ø­Ù„Ù‚Ø© Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©"""
        while self.is_monitoring:
            try:
                # Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù…
                system_metrics = self._collect_system_metrics()
                
                # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‡Ù†Ø§Ùƒ Ù…Ø´Ø§ÙƒÙ„
                if system_metrics['cpu_usage'] > 80 or system_metrics['memory_usage'] > 85:
                    logger.warning(f"âš ï¸  Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ø§Ù„ÙŠ Ù„Ù„Ù…ÙˆØ§Ø±Ø¯: CPU {system_metrics['cpu_usage']}%, Memory {system_metrics['memory_usage']}%")
                
                time.sleep(60)  # Ø§Ù„ØªØ­Ù‚Ù‚ ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚Ø©
                
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡: {e}")
                time.sleep(300)  # Ø§Ù†ØªØ¸Ø§Ø± 5 Ø¯Ù‚Ø§Ø¦Ù‚ Ø¹Ù†Ø¯ Ø§Ù„Ø®Ø·Ø£
    
    def _collect_system_metrics(self) -> Dict[str, float]:
        """Ø¬Ù…Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù†Ø¸Ø§Ù…"""
        try:
            cpu_usage = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            return {
                'cpu_usage': cpu_usage,
                'memory_usage': memory.percent,
                'memory_available': memory.available / 1024 / 1024 / 1024,  # GB
                'disk_usage': disk.percent,
                'disk_free': disk.free / 1024 / 1024 / 1024  # GB
            }
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù…Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù†Ø¸Ø§Ù…: {e}")
            return {}

# Ù…ØµØ­Ø­ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
def query_optimizer(decorated_function):
    """Ù…ØµØ­Ø­ Ù„ØªØ­Ø³ÙŠÙ† ÙˆØªØªØ¨Ø¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª"""
    @wraps(decorated_function)
    async def async_wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = await decorated_function(*args, **kwargs)
            execution_time = time.time() - start_time
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¨Ø·ÙŠØ¦Ø§Ù‹ Ø£Ùˆ Ù…Ø¹Ù‚Ø¯Ø§Ù‹
            if execution_time > 0.5:  # Ø¹ØªØ¨Ø© Ø§Ù„ÙˆÙ‚Øª
                query_text = kwargs.get('query') or args[0] if args else 'unknown'
                optimizer = AdvancedQueryOptimizer()
                optimizer.analyze_query(query_text, execution_time)
            
            return result
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {e}")
            raise
    
    @wraps(decorated_function)
    def sync_wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = decorated_function(*args, **kwargs)
            execution_time = time.time() - start_time
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¨Ø·ÙŠØ¦Ø§Ù‹ Ø£Ùˆ Ù…Ø¹Ù‚Ø¯Ø§Ù‹
            if execution_time > 0.5:  # Ø¹ØªØ¨Ø© Ø§Ù„ÙˆÙ‚Øª
                query_text = kwargs.get('query') or args[0] if args else 'unknown'
                optimizer = AdvancedQueryOptimizer()
                optimizer.analyze_query(query_text, execution_time)
            
            return result
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {e}")
            raise
    
    return async_wrapper if asyncio.iscoroutinefunction(decorated_function) else sync_wrapper

# Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ù„Ù„Ø®Ø¯Ù…Ø©
query_optimizer_service = AdvancedQueryOptimizer()

# Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙˆØ§ÙÙ‚
def analyze_query(query_text: str, execution_time: float, **kwargs) -> QueryAnalysis:
    """Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙˆØ§ÙÙ‚ Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…"""
    return query_optimizer_service.analyze_query(query_text, execution_time, **kwargs)

def get_performance_report(hours: int = 24) -> PerformanceReport:
    """Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙˆØ§ÙÙ‚ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    return query_optimizer_service.get_comprehensive_performance_report(hours)

def optimize_query(query_text: str, **kwargs) -> Dict[str, Any]:
    """Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙˆØ§ÙÙ‚ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…"""
    return query_optimizer_service.optimize_query_execution(query_text, kwargs)

if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
    async def test_advanced_optimizer():
        print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø³Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…...")
        
        # Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ø®ØªØ¨Ø§Ø±
        test_queries = [
            "SELECT * FROM users WHERE age > 30 AND city = 'New York'",
            "SELECT u.name, o.amount FROM users u JOIN orders o ON u.id = o.user_id WHERE o.amount > 1000",
            "SELECT COUNT(*), department FROM employees GROUP BY department HAVING COUNT(*) > 5",
            "SELECT * FROM products WHERE name LIKE '%apple%' OR description LIKE '%fruit%'"
        ]
        
        for i, query in enumerate(test_queries):
            print(f"\n--- Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ø®ØªØ¨Ø§Ø± {i+1} ---")
            print(f"Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {query}")
            
            analysis = query_optimizer_service.analyze_query(query, 0.5, 100)
            print(f"Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {analysis.query_type.value}")
            print(f"Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ­Ø³ÙŠÙ†: {analysis.optimization_level.value}")
            print(f"Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†: {analysis.optimization_suggestions[:3]}")
        
        # ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡
        report = query_optimizer_service.get_comprehensive_performance_report(1)
        print(f"\nğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡:")
        print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª: {report.total_queries}")
        print(f"Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø¨Ø·ÙŠØ¦Ø©: {report.slow_queries}")
        print(f"Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„ØªÙ†ÙÙŠØ°: {report.avg_execution_time:.4f} Ø«Ø§Ù†ÙŠØ©")
    
    import asyncio
    asyncio.run(test_advanced_optimizer())
