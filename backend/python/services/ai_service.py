# backend/python/services/ai_service.py
"""
ğŸ§  Ø®Ø¯Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© - ØªØºØ·ÙŠØ© ÙƒØ§Ù…Ù„Ø© Ù„Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ
Ø§Ù„Ø¥ØµØ¯Ø§Ø±: 3.0.0 | Ø§Ù„Ù…Ø·ÙˆØ±: Akraa Trading Team
"""

import asyncio
import logging
import os
import sys
import time
import traceback
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import uuid
import random
from decimal import Decimal

# AI & ML Libraries
import numpy as np
import pandas as pd
import joblib
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.utils import class_weight
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.model_selection import train_test_split, TimeSeriesSplit
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model, save_model
from tensorflow.keras.layers import (
    LSTM, Dense, Dropout, Bidirectional, Conv1D, MaxPooling1D, 
    Flatten, BatchNormalization, Activation, LeakyReLU
)
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.callbacks import (
    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, 
    TensorBoard, Callback
)
from tensorflow.keras.regularizers import l1_l2, l2
from tensorflow.keras.utils import to_categorical

# Technical Analysis
import talib
import pandas_ta as ta
from scipy import stats, signal
import pytz

# Advanced Features
from collections import deque, Counter
import warnings
warnings.filterwarnings('ignore')

# Custom Imports
from models.trading_models import *

logger = logging.getLogger(__name__)

class AdvancedAIService:
    """Ø®Ø¯Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© - ØªØºØ·ÙŠØ© ÙƒØ§Ù…Ù„Ø© Ù„Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ"""
    
    def __init__(self):
        self.model_base_dir = "ai_models"
        self.lookback = 120
        self.sequence_length = 80
        self.prediction_horizon = 5
        
        # Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„ÙƒÙ„ Ø±Ù…Ø²
        self.symbol_models: Dict[str, tf.keras.Model] = {}
        self.symbol_scalers: Dict[str, MinMaxScaler] = {}
        self.symbol_data: Dict[str, deque] = {}
        self.model_versions: Dict[str, str] = {}
        
        # ØªØªØ¨Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡
        self.model_performance: Dict[str, Dict] = {}
        self.prediction_history: Dict[str, List] = {}
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ù† Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ
        self.ai_config = self._load_ai_config()
        self.technical_indicators = self._get_technical_indicators()
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø³Ø¨Ù‚Ø©
        self._ensure_directories()
        
        logger.info("ğŸ§  ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø®Ø¯Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©")

    def _load_ai_config(self):
        """ØªØ­Ù…ÙŠÙ„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ù† Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ"""
        return {
            'high_confidence_threshold': 0.65,
            'medium_confidence_threshold': 0.45,
            'min_training_samples': 400,
            'max_training_samples': 2000,
            'validation_split': 0.2,
            'early_stopping_patience': 15,
            'learning_rate': 0.0008,
            'batch_size': 48,
            'epochs': 80,
            'class_balance_boost': 1.3,
            'feature_engineering': True,
            'ensemble_learning': True,
            'transfer_learning': True
        }

    def _get_technical_indicators(self):
        """Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ© Ù…Ù† Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ"""
        return {
            'trend': [
                'macd', 'macd_signal', 'macd_hist', 'adx', 'adx_pos', 'adx_neg',
                'cci', 'rsi', 'stoch_k', 'stoch_d', 'williams_r', 'uo', 'ao'
            ],
            'momentum': [
                'mom', 'roc', 'ppo', 'pvo', 'kst', 'kst_sig', 'dpo', 'slope'
            ],
            'volatility': [
                'bb_upper', 'bb_middle', 'bb_lower', 'bb_width', 'bb_pct',
                'atr', 'natr', 'rvi', 'ui'
            ],
            'volume': [
                'obv', 'cmf', 'mfi', 'adi', 'eom', 'vpt', 'nvi', 'pvi'
            ],
            'cycle': [
                'ht_dcperiod', 'ht_dcphase', 'ht_phasor_inphase', 'ht_phasor_quadrature',
                'ht_sine', 'ht_leadsine', 'ht_trendmode'
            ],
            'pattern': [
                'cdl_doji', 'cdl_hammer', 'cdl_engulfing', 'cdl_morningstar',
                'cdl_eveningstar', 'cdl_harami', 'cdl_piercing'
            ]
        }

    def _ensure_directories(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø©"""
        os.makedirs(self.model_base_dir, exist_ok=True)
        os.makedirs(f"{self.model_base_dir}/training_logs", exist_ok=True)
        os.makedirs(f"{self.model_base_dir}/model_checkpoints", exist_ok=True)

    async def initialize_symbol_model(self, symbol: str) -> bool:
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø±Ù…Ø² Ø§Ù„Ù…Ø­Ø¯Ø¯"""
        try:
            symbol_key = symbol.replace('/', '_')
            model_loaded = await self._load_existing_model(symbol)
            
            if not model_loaded:
                logger.info(f"ğŸ”„ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø¯ÙŠØ¯ Ù„Ù€ {symbol}")
                # Ø³ÙŠØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù†Ø¯ ØªÙˆÙØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                self.symbol_data[symbol] = deque(maxlen=1000)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù€ {symbol}: {traceback.format_exc()}")
            return False

    async def _load_existing_model(self, symbol: str) -> bool:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹"""
        try:
            symbol_key = symbol.replace('/', '_')
            model_path = f"{self.model_base_dir}/{symbol_key}/ai_trading_model.h5"
            scaler_path = f"{self.model_base_dir}/{symbol_key}/ai_scaler.pkl"
            
            if os.path.exists(model_path) and os.path.exists(scaler_path):
                self.symbol_models[symbol] = load_model(model_path)
                self.symbol_scalers[symbol] = joblib.load(scaler_path)
                
                # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø©
                performance_path = f"{self.model_base_dir}/{symbol_key}/performance.json"
                if os.path.exists(performance_path):
                    import json
                    with open(performance_path, 'r') as f:
                        self.model_performance[symbol] = json.load(f)
                
                logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù„Ù€ {symbol}")
                return True
                
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù€ {symbol}: {str(e)}")
        
        return False

    async def train_ai_model(self, symbol: str, ohlcv_data: List[List[float]], 
                           force_retrain: bool = False) -> bool:
        """ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ - Ø§Ù„ØªØºØ·ÙŠØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù…Ù† Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ"""
        try:
            if len(ohlcv_data) < self.ai_config['min_training_samples']:
                logger.warning(f"âš ï¸ Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ù€ {symbol}: {len(ohlcv_data)} < {self.ai_config['min_training_samples']}")
                return False

            logger.info(f"ğŸ¯ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù€ {symbol} Ù…Ø¹ {len(ohlcv_data)} Ø¹ÙŠÙ†Ø©")

            # 1. ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
            df = self._prepare_advanced_features(ohlcv_data, symbol)
            if df is None or len(df) < 100:
                return False

            # 2. Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‡Ø¯Ù Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª
            X, y, feature_names = self._create_advanced_target(df, symbol)
            if X is None or len(X) < 100:
                return False

            # 3. ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ø²Ù…Ù†ÙŠ
            X_train, X_val, y_train, y_val = self._time_series_split(X, y)

            # 4. Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
            class_weights = self._calculate_advanced_class_weights(y_train)

            # 5. Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
            model = self._build_advanced_model(input_shape=(X_train.shape[1], X_train.shape[2]))
            
            # 6. ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ callbacks Ù…ØªÙ‚Ø¯Ù…Ø©
            training_success = await self._advanced_model_training(
                model, X_train, y_train, X_val, y_val, class_weights, symbol
            )

            if training_success:
                # 7. ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
                evaluation_results = await self._comprehensive_model_evaluation(
                    model, X_val, y_val, symbol
                )
                
                # 8. Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                await self._save_model_and_artifacts(model, symbol, feature_names, evaluation_results)
                
                logger.info(f"âœ… Ø§ÙƒØªÙ…Ù„ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù€ {symbol} Ø¨Ù†Ø¬Ø§Ø­")
                return True
            else:
                logger.error(f"âŒ ÙØ´Ù„ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù€ {symbol}")
                return False

        except Exception as e:
            logger.error(f"ğŸ’¥ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù€ {symbol}: {traceback.format_exc()}")
            return False

    def _prepare_advanced_features(self, ohlcv_data: List[List[float]], symbol: str) -> pd.DataFrame:
        """ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø³Ù…Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ù…Ù† Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ"""
        try:
            df = pd.DataFrame(ohlcv_data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df.set_index('timestamp', inplace=True)
            
            # Ø§Ù„Ø³Ù…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù…Ù† Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ
            df = self._enhance_temporal_features(df)
            
            # Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
            df = self._add_technical_indicators(df)
            
            # Ø§Ù„Ø³Ù…Ø§Øª Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ©
            df = self._add_statistical_features(df)
            
            # Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø´Ù…ÙˆØ¹
            df = self._add_candlestick_patterns(df)
            
            # Ø³Ù…Ø§Øª Ø§Ù„Ø²Ø®Ù… ÙˆØ§Ù„ØªÙ‚Ù„Ø¨
            df = self._add_momentum_volatility_features(df)
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
            df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ØªØ·Ø±ÙØ©
            df = self._remove_outliers(df)
            
            logger.info(f"ğŸ“Š ØªÙ… ØªØ­Ø¶ÙŠØ± {len(df.columns)} Ø³Ù…Ø© Ù„Ù€ {symbol}")
            return df
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø³Ù…Ø§Øª Ù„Ù€ {symbol}: {str(e)}")
            return None

    def _enhance_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø³Ù…Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ù…Ù† Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ"""
        try:
            # Ø§Ù„Ø³Ù…Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            df['price_momentum'] = df['close'].pct_change(5)
            df['volume_trend'] = df['volume'].rolling(10).mean()
            df['volatility'] = df['close'].rolling(20).std()
            
            # Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª Ø§Ù„Ù…ØªØ­Ø±ÙƒØ© Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
            for period in [5, 10, 20, 50, 100]:
                df[f'sma_{period}'] = talib.SMA(df['close'], timeperiod=period)
                df[f'ema_{period}'] = talib.EMA(df['close'], timeperiod=period)
                df[f'price_vs_sma_{period}'] = (df['close'] - df[f'sma_{period}']) / df[f'sma_{period}']
            
            # Ù‚ÙˆØ© Ø§Ù„Ø§ØªØ¬Ø§Ù‡
            df['trend_strength'] = talib.ADX(df['high'], df['low'], df['close'], timeperiod=14)
            df['momentum'] = talib.MOM(df['close'], timeperiod=10)
            
            # ØªÙ‚Ù„Ø¨Ø§Øª Ø§Ù„Ø­Ø¬Ù…
            df['volume_volatility'] = df['volume'].rolling(20).std()
            df['volume_sma_ratio'] = df['volume'] / df['volume'].rolling(20).mean()
            
            return df
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø³Ù…Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ©: {str(e)}")
            return df

    def _add_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ù…Ù† Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ"""
        try:
            # RSI Ø¨Ù…Ø®ØªÙ„Ù Ø§Ù„ÙØªØ±Ø§Øª
            for period in [6, 14, 21]:
                df[f'rsi_{period}'] = talib.RSI(df['close'], timeperiod=period)
            
            # MACD
            macd, macd_signal, macd_hist = talib.MACD(df['close'])
            df['macd'] = macd
            df['macd_signal'] = macd_signal
            df['macd_hist'] = macd_hist
            
            # Bollinger Bands
            bb_upper, bb_middle, bb_lower = talib.BBANDS(df['close'])
            df['bb_upper'] = bb_upper
            df['bb_middle'] = bb_middle
            df['bb_lower'] = bb_lower
            df['bb_width'] = (bb_upper - bb_lower) / bb_middle
            df['bb_position'] = (df['close'] - bb_lower) / (bb_upper - bb_lower)
            
            # Stochastic
            stoch_k, stoch_d = talib.STOCH(df['high'], df['low'], df['close'])
            df['stoch_k'] = stoch_k
            df['stoch_d'] = stoch_d
            
            # ATR
            df['atr'] = talib.ATR(df['high'], df['low'], df['close'])
            
            # OBV
            df['obv'] = talib.OBV(df['close'], df['volume'])
            
            # CCI
            df['cci'] = talib.CCI(df['high'], df['low'], df['close'])
            
            # Williams %R
            df['williams_r'] = talib.WILLR(df['high'], df['low'], df['close'])
            
            # ADX
            df['adx'] = talib.ADX(df['high'], df['low'], df['close'])
            
            return df
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ÙÙ†ÙŠØ©: {str(e)}")
            return df

    def _add_statistical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø³Ù…Ø§Øª Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ©"""
        try:
            # Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ
            df['returns'] = df['close'].pct_change()
            df['volatility_1d'] = df['returns'].rolling(20).std()
            df['volatility_5d'] = df['returns'].rolling(100).std()
            
            # Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù
            df['skewness'] = df['returns'].rolling(50).skew()
            df['kurtosis'] = df['returns'].rolling(50).kurtosis()
            
            # Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ø°Ø§ØªÙŠ
            df['autocorr_1'] = df['returns'].rolling(50).apply(lambda x: x.autocorr(lag=1), raw=False)
            df['autocorr_5'] = df['returns'].rolling(50).apply(lambda x: x.autocorr(lag=5), raw=False)
            
            # Hurst Exponent (ØªÙ‚Ø±ÙŠØ¨ÙŠ)
            df['hurst'] = df['returns'].rolling(100).apply(self._calculate_hurst, raw=False)
            
            return df
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø³Ù…Ø§Øª Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ©: {str(e)}")
            return df

    def _add_candlestick_patterns(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ø¥Ø¶Ø§ÙØ© Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø´Ù…ÙˆØ¹ Ø§Ù„ÙŠØ§Ø¨Ø§Ù†ÙŠØ©"""
        try:
            # Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø´Ù…ÙˆØ¹ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            patterns = [
                'CDLDOJI', 'CDLHAMMER', 'CDLENGULFING', 'CDLMORNINGSTAR',
                'CDLEVENINGSTAR', 'CDLHARAMI', 'CDLPIERCING', 'CDLDARKCLOUDCOVER',
                'CDLSHOOTINGSTAR', 'CDL3WHITESOLDIERS', 'CDL3BLACKCROWS'
            ]
            
            for pattern in patterns:
                try:
                    pattern_func = getattr(talib, pattern)
                    df[pattern.lower()] = pattern_func(df['open'], df['high'], df['low'], df['close'])
                except Exception as e:
                    logger.debug(f"âš ï¸ ØªØ¹Ø°Ø± Ø¥Ø¶Ø§ÙØ© Ù†Ù…Ø· {pattern}: {str(e)}")
                    continue
            
            return df
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¶Ø§ÙØ© Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø´Ù…ÙˆØ¹: {str(e)}")
            return df

    def _add_momentum_volatility_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ø¥Ø¶Ø§ÙØ© Ø³Ù…Ø§Øª Ø§Ù„Ø²Ø®Ù… ÙˆØ§Ù„ØªÙ‚Ù„Ø¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        try:
            # Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø²Ø®Ù…
            df['roc_5'] = talib.ROC(df['close'], timeperiod=5)
            df['roc_10'] = talib.ROC(df['close'], timeperiod=10)
            df['roc_20'] = talib.ROC(df['close'], timeperiod=20)
            
            # TRIX
            df['trix'] = talib.TRIX(df['close'])
            
            # Ultimate Oscillator
            df['uo'] = talib.ULTOSC(df['high'], df['low'], df['close'])
            
            # Chaikin Oscillator
            df['adosc'] = talib.ADOSC(df['high'], df['low'], df['close'], df['volume'])
            
            # Money Flow Index
            df['mfi'] = talib.MFI(df['high'], df['low'], df['close'], df['volume'])
            
            return df
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¶Ø§ÙØ© Ø³Ù…Ø§Øª Ø§Ù„Ø²Ø®Ù…: {str(e)}")
            return df

    def _remove_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ØªØ·Ø±ÙØ©"""
        try:
            for column in df.select_dtypes(include=[np.number]).columns:
                if df[column].std() > 0:  # ØªØ¬Ù†Ø¨ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø«Ø§Ø¨ØªØ©
                    q1 = df[column].quantile(0.05)
                    q3 = df[column].quantile(0.95)
                    iqr = q3 - q1
                    lower_bound = q1 - 1.5 * iqr
                    upper_bound = q3 + 1.5 * iqr
                    df[column] = np.clip(df[column], lower_bound, upper_bound)
            return df
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ØªØ·Ø±ÙØ©: {str(e)}")
            return df

    def _calculate_hurst(self, returns):
        """Ø­Ø³Ø§Ø¨ Hurst Exponent (ØªÙ‚Ø±ÙŠØ¨ÙŠ)"""
        try:
            if len(returns) < 2:
                return 0.5
            lags = range(2, min(20, len(returns)))
            tau = [np.std(np.subtract(returns[lag:], returns[:-lag])) for lag in lags]
            poly = np.polyfit(np.log(lags), np.log(tau), 1)
            return poly[0]
        except:
            return 0.5

    def _create_advanced_target(self, df: pd.DataFrame, symbol: str) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‡Ø¯Ù Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ù…Ù† Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ"""
        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… multiple time horizons Ù„Ù„ØªÙ†Ø¨Ø¤
            horizons = [1, 3, 5, 10]
            future_returns = []
            
            for horizon in horizons:
                future_price = df['close'].shift(-horizon)
                returns = (future_price - df['close']) / df['close']
                future_returns.append(returns)
            
            # Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¹ÙˆØ§Ø¦Ø¯ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©
            avg_future_returns = pd.concat(future_returns, axis=1).mean(axis=1)
            
            # Ø¥Ù†Ø´Ø§Ø¡ ÙØ¦Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©
            conditions = [
                avg_future_returns > 0.02,   # ØµØ¹ÙˆØ¯ Ù‚ÙˆÙŠ
                avg_future_returns > 0.005,  # ØµØ¹ÙˆØ¯ Ù…Ø¹ØªØ¯Ù„
                avg_future_returns < -0.02,  # Ù‡Ø¨ÙˆØ· Ù‚ÙˆÙŠ
                avg_future_returns < -0.005  # Ù‡Ø¨ÙˆØ· Ù…Ø¹ØªØ¯Ù„
            ]
            choices = [2, 1, -2, -1]  # 2: ØµØ¹ÙˆØ¯ Ù‚ÙˆÙŠ, 1: ØµØ¹ÙˆØ¯ Ù…Ø¹ØªØ¯Ù„, -1: Ù‡Ø¨ÙˆØ· Ù…Ø¹ØªØ¯Ù„, -2: Ù‡Ø¨ÙˆØ· Ù‚ÙˆÙŠ, 0: Ù…Ø­Ø§ÙŠØ¯
            
            df['target'] = np.select(conditions, choices, default=0)
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨
            feature_columns = [col for col in df.columns if col not in ['target', 'future'] and not col.startswith('future_')]
            X = df[feature_columns].values
            y = df['target'].values
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø°Ø§Øª Ø§Ù„Ù‚ÙŠÙ… NaN
            valid_indices = ~np.isnan(X).any(axis=1) & ~np.isnan(y)
            X = X[valid_indices]
            y = y[valid_indices]
            
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ sequences
            X_sequences, y_sequences = [], []
            for i in range(self.sequence_length, len(X)):
                X_sequences.append(X[i-self.sequence_length:i])
                y_sequences.append(y[i])
            
            if len(X_sequences) < 100:
                logger.warning(f"âš ï¸ Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù„Ù€ {symbol}")
                return None, None, []
            
            return np.array(X_sequences), np.array(y_sequences), feature_columns
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‡Ø¯Ù Ù„Ù€ {symbol}: {str(e)}")
            return None, None, []

    def _time_series_split(self, X: np.ndarray, y: np.ndarray) -> Tuple:
        """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ø²Ù…Ù†ÙŠ"""
        try:
            split_index = int(len(X) * (1 - self.ai_config['validation_split']))
            
            X_train = X[:split_index]
            X_val = X[split_index:]
            y_train = y[:split_index]
            y_val = y[split_index:]
            
            return X_train, X_val, y_train, y_val
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {str(e)}")
            raise

    def _calculate_advanced_class_weights(self, y: np.ndarray) -> Dict[int, float]:
        """Ø­Ø³Ø§Ø¨ Ø£ÙˆØ²Ø§Ù† Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        try:
            class_counts = Counter(y)
            total_samples = len(y)
            n_classes = len(class_counts)
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… inverse frequency Ù…Ø¹ smoothing
            class_weights = {}
            for class_label, count in class_counts.items():
                # Ø¥Ø¶Ø§ÙØ© smoothing Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù‚ØµÙˆÙ‰
                weight = total_samples / (n_classes * count)
                # ØªØ·Ø¨ÙŠÙ‚ class_balance_boost Ù…Ù† Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
                weight = weight ** self.ai_config['class_balance_boost']
                class_weights[class_label] = min(weight, 10.0)  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ Ù„Ù„Ø£ÙˆØ²Ø§Ù†
            
            logger.info(f"âš–ï¸ Ø£ÙˆØ²Ø§Ù† Ø§Ù„ÙØ¦Ø§Øª: {class_weights}")
            return class_weights
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø£ÙˆØ²Ø§Ù† Ø§Ù„ÙØ¦Ø§Øª: {str(e)}")
            return {0: 1.0, 1: 1.0, -1: 1.0, 2: 1.0, -2: 1.0}

    def _build_advanced_model(self, input_shape: Tuple[int, int]) -> tf.keras.Model:
        """Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù…Ù† Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ"""
        try:
            model = Sequential([
                # Ø·Ø¨Ù‚Ø© Conv1D Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø­Ù„ÙŠØ©
                Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),
                BatchNormalization(),
                MaxPooling1D(pool_size=2),
                Dropout(0.2),
                
                # Ø·Ø¨Ù‚Ø§Øª LSTM Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„Ø§ØªØ¬Ø§Ù‡
                Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.001))),
                BatchNormalization(),
                Dropout(0.3),
                
                Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.001))),
                BatchNormalization(),
                Dropout(0.3),
                
                Bidirectional(LSTM(32, kernel_regularizer=l2(0.001))),
                BatchNormalization(),
                Dropout(0.3),
                
                # Ø·Ø¨Ù‚Ø§Øª ÙƒØ«ÙŠÙØ© Ù…ØªÙ‚Ø¯Ù…Ø©
                Dense(128, kernel_regularizer=l2(0.001)),
                BatchNormalization(),
                LeakyReLU(alpha=0.1),
                Dropout(0.4),
                
                Dense(64, kernel_regularizer=l2(0.001)),
                BatchNormalization(),
                LeakyReLU(alpha=0.1),
                Dropout(0.4),
                
                # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
                Dense(5, activation='softmax')  # 5 ÙØ¦Ø§Øª
            ])
            
            # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            model.compile(
                optimizer=Adam(learning_rate=self.ai_config['learning_rate']),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy', 'sparse_categorical_accuracy']
            )
            
            logger.info("âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ø¨Ù†Ø¬Ø§Ø­")
            return model
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {traceback.format_exc()}")
            raise

    async def _advanced_model_training(self, model: tf.keras.Model, X_train: np.ndarray, 
                                     y_train: np.ndarray, X_val: np.ndarray, 
                                     y_val: np.ndarray, class_weights: Dict, 
                                     symbol: str) -> bool:
        """Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ù†Ù…ÙˆØ°Ø¬"""
        try:
            # Callbacks Ù…ØªÙ‚Ø¯Ù…Ø©
            callbacks = [
                EarlyStopping(
                    monitor='val_loss',
                    patience=self.ai_config['early_stopping_patience'],
                    restore_best_weights=True,
                    verbose=1
                ),
                ModelCheckpoint(
                    f"{self.model_base_dir}/model_checkpoints/{symbol.replace('/', '_')}_best.h5",
                    monitor='val_accuracy',
                    save_best_only=True,
                    save_weights_only=False,
                    verbose=1
                ),
                ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=0.5,
                    patience=8,
                    min_lr=1e-7,
                    verbose=1
                ),
                TensorBoard(
                    log_dir=f"{self.model_base_dir}/training_logs/{symbol.replace('/', '_')}",
                    histogram_freq=1,
                    write_graph=True,
                    write_images=True
                )
            ]
            
            # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            history = model.fit(
                X_train, y_train,
                epochs=self.ai_config['epochs'],
                batch_size=self.ai_config['batch_size'],
                validation_data=(X_val, y_val),
                callbacks=callbacks,
                class_weight=class_weights,
                verbose=1,
                shuffle=False  # Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ø²Ù…Ù†ÙŠ
            )
            
            # Ø­ÙØ¸ ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            self._save_training_history(history, symbol)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù€ {symbol}: {traceback.format_exc()}")
            return False

    async def _comprehensive_model_evaluation(self, model: tf.keras.Model, X_val: np.ndarray, 
                                            y_val: np.ndarray, symbol: str) -> Dict[str, Any]:
        """ØªÙ‚ÙŠÙŠÙ… Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬"""
        try:
            # Ø§Ù„ØªÙ†Ø¨Ø¤
            y_pred_proba = model.predict(X_val, verbose=0)
            y_pred = np.argmax(y_pred_proba, axis=1)
            
            # Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª
            accuracy = accuracy_score(y_val, y_pred)
            precision = precision_score(y_val, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_val, y_pred, average='weighted', zero_division=0)
            f1 = f1_score(y_val, y_pred, average='weighted', zero_division=0)
            
            # ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ
            class_report = classification_report(y_val, y_pred, output_dict=True, zero_division=0)
            
            # Ø«Ù‚Ø© Ø§Ù„ØªÙ†Ø¨Ø¤
            prediction_confidence = np.max(y_pred_proba, axis=1).mean()
            
            results = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'prediction_confidence': prediction_confidence,
                'class_distribution': dict(Counter(y_val)),
                'classification_report': class_report,
                'evaluation_timestamp': datetime.utcnow().isoformat(),
                'model_version': '3.0.0'
            }
            
            logger.info(f"ğŸ“Š ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù€ {symbol}: Ø¯Ù‚Ø© {accuracy:.4f}, F1 {f1:.4f}")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {str(e)}")
            return {}

    async def _save_model_and_artifacts(self, model: tf.keras.Model, symbol: str, 
                                      feature_names: List[str], evaluation_results: Dict):
        """Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©"""
        try:
            symbol_key = symbol.replace('/', '_')
            symbol_dir = f"{self.model_base_dir}/{symbol_key}"
            os.makedirs(symbol_dir, exist_ok=True)
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            model.save(f"{symbol_dir}/ai_trading_model.h5")
            
            # Ø­ÙØ¸ Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
            if symbol in self.symbol_scalers:
                joblib.dump(self.symbol_scalers[symbol], f"{symbol_dir}/ai_scaler.pkl")
            
            # Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
            import json
            with open(f"{symbol_dir}/performance.json", 'w') as f:
                json.dump(evaluation_results, f, indent=2)
            
            # Ø­ÙØ¸ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø³Ù…Ø§Øª
            with open(f"{symbol_dir}/feature_names.json", 'w') as f:
                json.dump(feature_names, f, indent=2)
            
            # Ø­ÙØ¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            model_config = {
                'lookback': self.lookback,
                'sequence_length': self.sequence_length,
                'prediction_horizon': self.prediction_horizon,
                'model_version': '3.0.0',
                'training_timestamp': datetime.utcnow().isoformat(),
                'feature_count': len(feature_names)
            }
            with open(f"{symbol_dir}/model_config.json", 'w') as f:
                json.dump(model_config, f, indent=2)
                
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {str(e)}")

    def _save_training_history(self, history, symbol: str):
        """Ø­ÙØ¸ ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        try:
            symbol_key = symbol.replace('/', '_')
            history_path = f"{self.model_base_dir}/training_logs/{symbol_key}_history.json"
            
            import json
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ… Ø¥Ù„Ù‰ Ø£Ù†ÙˆØ§Ø¹ Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ³Ø¬ÙŠÙ„ JSON
            history_dict = {}
            for key, values in history.history.items():
                history_dict[key] = [float(val) for val in values]
            
            with open(history_path, 'w') as f:
                json.dump(history_dict, f, indent=2)
                
        except Exception as e:
            logger.warning(f"âš ï¸ ØªØ¹Ø°Ø± Ø­ÙØ¸ ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {str(e)}")

    async def predict(self, symbol: str, ohlcv_data: List[List[float]]) -> AIPrediction:
        """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… - Ø§Ù„ØªØºØ·ÙŠØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù…Ù† Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ"""
        try:
            if symbol not in self.symbol_models or self.symbol_models[symbol] is None:
                await self.initialize_symbol_model(symbol)
            
            if (symbol not in self.symbol_models or 
                self.symbol_models[symbol] is None or 
                len(ohlcv_data) < self.sequence_length):
                
                return self._create_fallback_prediction(symbol)
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªÙ†Ø¨Ø¤
            df = self._prepare_advanced_features(ohlcv_data, symbol)
            if df is None:
                return self._create_fallback_prediction(symbol)
            
            feature_columns = [col for col in df.columns if not col.startswith('future_')]
            X = df[feature_columns].values
            
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
            if symbol in self.symbol_scalers:
                X = self.symbol_scalers[symbol].transform(X)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ³Ù„Ø³Ù„
            if len(X) < self.sequence_length:
                return self._create_fallback_prediction(symbol)
            
            X_sequence = np.array([X[-self.sequence_length:]])
            
            # Ø§Ù„ØªÙ†Ø¨Ø¤
            prediction_proba = self.symbol_models[symbol].predict(X_sequence, verbose=0)[0]
            predicted_class = np.argmax(prediction_proba)
            confidence = np.max(prediction_proba)
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙØ¦Ø© Ø¥Ù„Ù‰ Ø¥Ø´Ø§Ø±Ø©
            signal_map = {
                2: AIPredictionType.BUY,    # ØµØ¹ÙˆØ¯ Ù‚ÙˆÙŠ
                1: AIPredictionType.BUY,    # ØµØ¹ÙˆØ¯ Ù…Ø¹ØªØ¯Ù„
                0: AIPredictionType.HOLD,   # Ù…Ø­Ø§ÙŠØ¯
                -1: AIPredictionType.SELL,  # Ù‡Ø¨ÙˆØ· Ù…Ø¹ØªØ¯Ù„
                -2: AIPredictionType.SELL   # Ù‡Ø¨ÙˆØ· Ù‚ÙˆÙŠ
            }
            
            signal = signal_map.get(predicted_class, AIPredictionType.HOLD)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©
            current_indicators = self._get_current_indicators(df)
            
            # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªÙ†Ø¨Ø¤
            await self._record_prediction(symbol, signal, confidence, predicted_class)
            
            return AIPrediction(
                symbol=symbol,
                prediction=signal,
                confidence=float(confidence),
                timestamp=datetime.utcnow(),
                indicators=current_indicators,
                timeframe=TimeFrame.ONE_HOUR,
                model_version=self.model_versions.get(symbol, "3.0.0"),
                features_used=feature_columns
            )
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤ Ù„Ù€ {symbol}: {traceback.format_exc()}")
            return self._create_fallback_prediction(symbol)

    def _get_current_indicators(self, df: pd.DataFrame) -> Dict[str, float]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©"""
        try:
            return {
                'rsi': float(df['rsi_14'].iloc[-1]) if 'rsi_14' in df.columns else 50.0,
                'macd': float(df['macd'].iloc[-1]) if 'macd' in df.columns else 0.0,
                'macd_signal': float(df['macd_signal'].iloc[-1]) if 'macd_signal' in df.columns else 0.0,
                'bb_position': float(df['bb_position'].iloc[-1]) if 'bb_position' in df.columns else 0.5,
                'atr': float(df['atr'].iloc[-1]) if 'atr' in df.columns else 0.0,
                'volume_trend': float(df['volume_trend'].iloc[-1]) if 'volume_trend' in df.columns else 0.0,
                'volatility': float(df['volatility'].iloc[-1]) if 'volatility' in df.columns else 0.0,
                'trend_strength': float(df['trend_strength'].iloc[-1]) if 'trend_strength' in df.columns else 0.0
            }
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª: {str(e)}")
            return {}

    async def _record_prediction(self, symbol: str, signal: AIPredictionType, 
                               confidence: float, predicted_class: int):
        """ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªÙ†Ø¨Ø¤ Ù„Ù„ØªØªØ¨Ø¹"""
        try:
            if symbol not in self.prediction_history:
                self.prediction_history[symbol] = []
            
            prediction_record = {
                'timestamp': datetime.utcnow(),
                'signal': signal,
                'confidence': confidence,
                'predicted_class': predicted_class,
                'model_version': self.model_versions.get(symbol, "3.0.0")
            }
            
            self.prediction_history[symbol].append(prediction_record)
            
            # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ ÙÙ‚Ø· Ø¨Ø¢Ø®Ø± 100 ØªÙ†Ø¨Ø¤
            if len(self.prediction_history[symbol]) > 100:
                self.prediction_history[symbol] = self.prediction_history[symbol][-100:]
                
        except Exception as e:
            logger.warning(f"âš ï¸ ØªØ¹Ø°Ø± ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªÙ†Ø¨Ø¤: {str(e)}")

    def _create_fallback_prediction(self, symbol: str) -> AIPrediction:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ†Ø¨Ø¤ Ø§Ø­ØªÙŠØ§Ø·ÙŠ"""
        return AIPrediction(
            symbol=symbol,
            prediction=AIPredictionType.HOLD,
            confidence=0.5,
            timestamp=datetime.utcnow(),
            indicators={},
            timeframe=TimeFrame.ONE_HOUR,
            model_version="1.0",
            features_used=[]
        )

    async def get_model_performance(self, symbol: str) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        try:
            if symbol in self.model_performance:
                return self.model_performance[symbol]
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ù…Ù† Ø§Ù„Ù…Ù„Ù
            symbol_key = symbol.replace('/', '_')
            performance_path = f"{self.model_base_dir}/{symbol_key}/performance.json"
            
            if os.path.exists(performance_path):
                import json
                with open(performance_path, 'r') as f:
                    self.model_performance[symbol] = json.load(f)
                    return self.model_performance[symbol]
            
            return {}
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {str(e)}")
            return {}

    async def get_prediction_history(self, symbol: str, limit: int = 50) -> List[Dict]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ§Ø±ÙŠØ® Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª"""
        try:
            if symbol in self.prediction_history:
                return self.prediction_history[symbol][-limit:]
            return []
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ ØªØ§Ø±ÙŠØ® Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª: {str(e)}")
            return []

    async def analyze_market_sentiment(self, symbol: str, ohlcv_data: List[List[float]]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
        try:
            # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
            prediction = await self.predict(symbol, ohlcv_data)
            
            # ØªØ­Ù„ÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠ Ù„Ù„Ù…Ø´Ø§Ø¹Ø±
            df = self._prepare_advanced_features(ohlcv_data, symbol)
            if df is None:
                return {
                    'symbol': symbol,
                    'overall_sentiment': 'neutral',
                    'confidence': 0.5,
                    'timestamp': datetime.utcnow().isoformat()
                }
            
            # Ø­Ø³Ø§Ø¨ Ù…Ø´Ø§Ø¹Ø± Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯
            sentiment_scores = self._calculate_multi_dimension_sentiment(df)
            
            return {
                'symbol': symbol,
                'overall_sentiment': self._get_sentiment_label(sentiment_scores['overall']),
                'confidence': float(sentiment_scores['confidence']),
                'sentiment_scores': sentiment_scores,
                'ai_prediction': prediction.dict(),
                'timestamp': datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù„Ù€ {symbol}: {str(e)}")
            return {
                'symbol': symbol,
                'overall_sentiment': 'neutral',
                'confidence': 0.5,
                'timestamp': datetime.utcnow().isoformat()
            }

    def _calculate_multi_dimension_sentiment(self, df: pd.DataFrame) -> Dict[str, float]:
        """Ø­Ø³Ø§Ø¨ Ù…Ø´Ø§Ø¹Ø± Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯"""
        try:
            scores = {}
            
            # Ø²Ø®Ù… Ø§Ù„Ø³Ø¹Ø±
            if 'rsi_14' in df.columns:
                rsi = df['rsi_14'].iloc[-1]
                scores['momentum'] = 1.0 - abs(rsi - 50) / 50  # ÙƒÙ„Ù…Ø§ Ø§Ù‚ØªØ±Ø¨ Ù…Ù† 50 ÙƒÙ„Ù…Ø§ ÙƒØ§Ù† Ù…Ø­Ø§ÙŠØ¯Ø§Ù‹
            
            # Ù‚ÙˆØ© Ø§Ù„Ø§ØªØ¬Ø§Ù‡
            if 'adx' in df.columns:
                adx = df['adx'].iloc[-1]
                scores['trend_strength'] = min(adx / 50, 1.0)  # ØªØ·Ø¨ÙŠØ¹ Ø¨ÙŠÙ† 0 Ùˆ 1
            
            # Ø§Ù„ØªÙ‚Ù„Ø¨
            if 'volatility' in df.columns:
                volatility = df['volatility'].iloc[-1]
                avg_volatility = df['volatility'].mean()
                scores['volatility_sentiment'] = 1.0 - min(volatility / (avg_volatility * 2), 1.0)
            
            # Ø§Ù„Ø­Ø¬Ù…
            if 'volume_trend' in df.columns:
                volume_ratio = df['volume_trend'].iloc[-1] / df['volume_trend'].mean()
                scores['volume_sentiment'] = min(volume_ratio, 2.0) / 2.0
            
            # Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø±Ø¬Ø­
            overall = np.mean(list(scores.values())) if scores else 0.5
            confidence = np.std(list(scores.values())) if scores else 0.0
            
            return {
                'overall': float(overall),
                'confidence': float(confidence),
                'components': scores
            }
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {str(e)}")
            return {'overall': 0.5, 'confidence': 0.0, 'components': {}}

    def _get_sentiment_label(self, score: float) -> str:
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ù‚Ø§Ø· Ø¥Ù„Ù‰ ØªØ³Ù…ÙŠØ© Ù…Ø´Ø§Ø¹Ø±"""
        if score > 0.7:
            return "strong_bullish"
        elif score > 0.6:
            return "bullish"
        elif score > 0.4:
            return "neutral"
        elif score > 0.3:
            return "bearish"
        else:
            return "strong_bearish"

    async def get_ai_health_status(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© ØµØ­Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        try:
            status = {
                'total_models': len(self.symbol_models),
                'loaded_models': sum(1 for model in self.symbol_models.values() if model is not None),
                'model_performance': {},
                'prediction_activity': {},
                'system_status': 'healthy',
                'last_updated': datetime.utcnow().isoformat()
            }
            
            # Ø¬Ù…Ø¹ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            for symbol in self.symbol_models:
                if symbol in self.model_performance:
                    perf = self.model_performance[symbol]
                    status['model_performance'][symbol] = {
                        'accuracy': perf.get('accuracy', 0),
                        'f1_score': perf.get('f1_score', 0),
                        'last_training': perf.get('evaluation_timestamp', 'unknown')
                    }
                
                if symbol in self.prediction_history:
                    status['prediction_activity'][symbol] = len(self.prediction_history[symbol])
            
            return status
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ ØµØ­Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ: {str(e)}")
            return {
                'total_models': 0,
                'loaded_models': 0,
                'system_status': 'unhealthy',
                'error': str(e),
                'last_updated': datetime.utcnow().isoformat()
            }

# Ù†Ø³Ø®Ø© Ù…Ø¨Ø³Ø·Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø±ÙŠØ¹
class SimpleAIService:
    """Ø®Ø¯Ù…Ø© Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ø¨Ø³Ø·Ø©"""
    
    def __init__(self):
        self.advanced_service = AdvancedAIService()
    
    async def get_prediction(self, symbol: str, ohlcv_data: List[List[float]]) -> AIPrediction:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙ†Ø¨Ø¤ Ù…Ø¨Ø³Ø·"""
        return await self.advanced_service.predict(symbol, ohlcv_data)
    
    async def get_sentiment(self, symbol: str, ohlcv_data: List[List[float]]) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø³ÙˆÙ‚"""
        return await self.advanced_service.analyze_market_sentiment(symbol, ohlcv_data)

# Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø¹Ø§Ù„Ù…ÙŠØ©
ai_service = AdvancedAIService()